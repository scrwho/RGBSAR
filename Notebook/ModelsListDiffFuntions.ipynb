{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8057c3ae-7f2c-4913-937b-998e2238c920",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfe733-6631-4623-9ad8-f8050538deef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2547fb6a-0292-4f3d-894a-584d794b9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from typing import List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageFont\n",
    "from IPython.display import display\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "import pytest\n",
    "import tensorboard\n",
    "import visualkeras\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import colorsys\n",
    "import webcolors\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import GPUtil\n",
    "import psutil\n",
    "import pynvml\n",
    "import platform\n",
    "\n",
    "\n",
    "import netCDF4\n",
    "import netCDF4 as nc\n",
    "import graphviz\n",
    "import visualkeras\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.gridspec as gridspec\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from pycore.blocks import *\n",
    "from pycore.tikzeng import *\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers, regularizers, mixed_precision, backend as K\n",
    "from tensorflow.keras.utils import img_to_array, load_img, plot_model#, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.preprocessing import image#, ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50, ResNet101, InceptionResNetV2, VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "from keras.utils.np_utils import to_categorical \n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# from tensorflow.keras_sequential_ascii import keras2ascii  # Uncomment if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d203f5-5f35-429a-94e6-8d35b5a5353d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Useful Rmarkdown link\n",
    "- https://eddjberry.netlify.app/post/writing-your-thesis-with-bookdown/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169fb9f-7df0-4ec4-9010-f7f2a1d37c0d",
   "metadata": {},
   "source": [
    "get_system_memory, get_intel_gpu_memory, get_tpu_memory are missing from any of the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3485654-07a8-47a9-a13b-e3a300010057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## File I/O and OS\n",
    "- file_count - Counts files in a folder\n",
    "- get_file_list - Gets file paths recursively\n",
    "- get_dir_list - Gets directory paths recursively\n",
    "- FolderTree - Prints folder structure with file counts\n",
    "- FileTree - Prints folder and file structure\n",
    "- get_image_paths - Gets image paths from input\n",
    "- CreateDir - Create a directory if it does not exists\n",
    "- get_model_names - Extract the names of Models from list of saved models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6583698-47d2-48ec-aade-139447833b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDir(dir):\n",
    "    return os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def file_count(folder):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        count += len([f for f in files if os.path.isfile(os.path.join(root, f))])\n",
    "    return count\n",
    "\n",
    "def get_file_list(data_folder):\n",
    "    return [os.path.join(dirpath, file) for dirpath, dirnames, files in os.walk(data_folder) for file in files]\n",
    "\n",
    "def get_dir_list(data_folder):\n",
    "    return [os.path.join(dirpath, dirname) for dirpath, dirnames, _ in os.walk(data_folder) for dirname in dirnames]\n",
    "\n",
    "\n",
    "\n",
    "def FolderTree(FolderPath):\n",
    "    for root, dirs, files in os.walk(FolderPath):\n",
    "        level = root.count(os.sep) - FolderPath.count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        num_files = len(files)\n",
    "        print(f\"{indent}{os.path.basename(root)}/ ({num_files} file{'s' if num_files != 1 else ''})\")\n",
    "        \n",
    "\n",
    "def FileTree(FolderPath):\n",
    "    for root, dirs, files in os.walk(FolderPath):\n",
    "        level = root.count(os.sep) - FolderPath.count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        num_files = len(files)\n",
    "        print(f\"{indent}{os.path.basename(root)}/ ({num_files} file{'s' if num_files != 1 else ''})\")\n",
    "        for file in files:\n",
    "            file_indent = ' ' * 4 * (level+1)\n",
    "            print(f\"{file_indent}{file}\")\n",
    "\n",
    "            \n",
    "def get_image_paths(image_path):\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = [path for path in image_path if path.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def read_files(file_list):\n",
    "  \n",
    "  data = {}\n",
    "\n",
    "  for f in file_list:\n",
    "    # print(f)\n",
    "\n",
    "    name = os.path.basename(f)\n",
    "    file_type = os.path.splitext(name)[1]\n",
    "    \n",
    "    date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{4})', name)\n",
    "    if date_match:\n",
    "      date = date_match.group(1)\n",
    "      name = name.replace(f'_{date}', '')\n",
    "\n",
    "    name = name.replace('wind_turbine_', '')\n",
    "    name = name.replace(file_type, '')\n",
    "    name = name.replace('-','_')\n",
    "    \n",
    "    if f.endswith('.h5'):\n",
    "      model = load_model(f)\n",
    "      data[name] = model\n",
    "\n",
    "    elif f.endswith('.csv'):      \n",
    "      df = pd.read_csv(f)  \n",
    "      data[name] = df\n",
    "\n",
    "    elif f.endswith('.pkl'):\n",
    "      with open(f, 'rb') as pkl:\n",
    "        data[name] = pickle.load(pkl)\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_filesPrint(file_list):\n",
    "  \n",
    "  # data = {}\n",
    "\n",
    "  for f in file_list:\n",
    "    # print(f)\n",
    "\n",
    "    name = os.path.basename(f)\n",
    "    file_type = os.path.splitext(name)[1]\n",
    "    \n",
    "    date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{4})', name)\n",
    "    if date_match:\n",
    "      date = date_match.group(1)\n",
    "      name = name.replace(f'_{date}', '')\n",
    "\n",
    "    name = name.replace('wind_turbine_', '')\n",
    "    name = name.replace(file_type, '')\n",
    "    name = name.replace('-','_')\n",
    "    # print(f\"{name} = {f}\")\n",
    "    \n",
    "    if f.endswith('.h5'):\n",
    "      print(f\"{name} = load_model('{f}') \\ntf.keras.backend.clear_session()  # Clear GPU memory \\n\")\n",
    "      # model = load_model(f)\n",
    "      # data[name] = model\n",
    "\n",
    "    elif f.endswith('.csv'):      \n",
    "      print(f\"{name}_df = pd.read_csv('{f}')\")\n",
    "      # df = pd.read_csv(f)  \n",
    "      # data[name] = df\n",
    "\n",
    "    elif f.endswith('.pkl'):\n",
    "      print(f\"with open('{f}', 'rb') as pkl:\\n    {name} = pickle.load(pkl)\")\n",
    "      # with open(f, 'rb') as pkl:\n",
    "      #   data[name] = pickle.load(pkl)\n",
    "\n",
    "  # return data\n",
    "\n",
    "def get_model_names(filtered_list):\n",
    "    model_names = []\n",
    "\n",
    "    for f in filtered_list:\n",
    "        name = os.path.basename(f)\n",
    "        file_type = os.path.splitext(name)[1]\n",
    "        name, _ = os.path.splitext(name)\n",
    "\n",
    "        date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{4})', name)\n",
    "        if date_match:\n",
    "            date = date_match.group(1)\n",
    "            name = name.replace(f'_{date}', '')\n",
    "\n",
    "        name = name.replace('wind_turbine_', '').replace('_history_', '').replace('_history1', '').replace('-','_')\n",
    "        model_names.append(name)\n",
    "\n",
    "    return model_names\n",
    "\n",
    "\n",
    "def CreateDir(dir):\n",
    "    return os.makedirs(dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f251bba-b646-4cbf-8fcb-e763825d8fb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## System Info\n",
    "\n",
    "    get_system_memory - Gets system memory usage\n",
    "    get_intel_gpu_memory - Gets Intel GPU memory usage\n",
    "    get_tpu_memory - Gets TPU memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c540f98-529f-48a5-9684-93ed7daf82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_memory():\n",
    "    svmem = psutil.virtual_memory()\n",
    "    return {'memory_total': f\"{svmem.total/(1024**3):.2f} GB\", 'memory_used': f\"{svmem.used/(1024**3):.2f} GB\"}\n",
    "\n",
    "def get_intel_gpu_memory():\n",
    "    if platform.system() != 'Windows':\n",
    "        return None\n",
    "    meminfo = os.popen('wmic path win32_VideoController get AdapterRAM').read()\n",
    "    return {'memory_total': f\"{int(meminfo.split()[-1])/(1024**3):.2f} GB\"}\n",
    "\n",
    "def get_nvidia_gpu_memory():\n",
    "    cmd = 'nvidia-smi --query-gpu=memory.total,memory.used --format=csv,nounits,noheader'\n",
    "    output = subprocess.check_output(cmd, shell=True).decode('utf-8').strip()\n",
    "    gpu_memory = [line.split(',') for line in output.split('\\n')]\n",
    "    return [{'id': i, 'memory_total': f\"{int(mem[0])/1e3:.2f} GB\", 'memory_used': f\"{int(mem[1])/1e3:.2f} GB\"} for i, mem in enumerate(gpu_memory)]\n",
    "\n",
    "def get_tpu_memory():\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "        tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
    "        tpu_strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)\n",
    "        return {'memory_total': f\"{tpu_strategy.num_replicas_in_sync * 8:.2f} GB\"} # each TPU has 8GB memory\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_system_memory():\n",
    "\n",
    "  memory = {}\n",
    "  \n",
    "  svmem = psutil.virtual_memory()\n",
    "  memory['ram'] = {\n",
    "    'total': f\"{svmem.total/(1024**3):.2f} GB\",\n",
    "    'used': f\"{svmem.used/(1024**3):.2f} GB\"\n",
    "  }\n",
    "\n",
    "  if platform.system() == 'Windows':\n",
    "    meminfo = os.popen('wmic path win32_VideoController get AdapterRAM').read()  \n",
    "    memory['intel_gpu'] = {\n",
    "      'total': f\"{int(meminfo.split()[-1])/(1024**3):.2f} GB\"\n",
    "    }\n",
    "\n",
    "  cmd = 'nvidia-smi --query-gpu=memory.total,memory.used --format=csv,nounits,noheader' \n",
    "  output = subprocess.check_output(cmd, shell=True).decode('utf-8').strip()\n",
    "  gpu_memory = [line.split(',') for line in output.split('\\n')]\n",
    "  memory['nvidia_gpu'] = [{\n",
    "      'id': i, \n",
    "      'total': f\"{int(mem[0])/1e3:.2f} GB\",\n",
    "      'used': f\"{int(mem[1])/1e3:.2f} GB\"\n",
    "    } for i, mem in enumerate(gpu_memory)]\n",
    "\n",
    "  try:\n",
    "    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "    tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
    "    tpu_strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)\n",
    "    memory['tpu'] = {\n",
    "      'total': f\"{tpu_strategy.num_replicas_in_sync * 8:.2f} GB\" \n",
    "    }\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  return memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492bf7e-ee1e-4d40-b7dc-8db9ff9297ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Sampling/Manipulation/Preprocessing\n",
    "\n",
    "\n",
    "    sample_list - Samples elements filtering by keyword\n",
    "    random_sample - Randomly samples elements\n",
    "    get_random_samples - Samples files from sub-folders\n",
    "    generate_data - Generates image data using ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f404485f-9a97-46b9-84d0-88ba553e5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_list(trend, lists, size):\n",
    "    data_list = [file for file in lists if trend in file]\n",
    "    return random.sample(data_list, min(size, len(data_list)))\n",
    "\n",
    "def random_sample(elements, n, seed=1234, replace=0):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    if replace:\n",
    "        return random.choices(elements, k=n)\n",
    "    else:\n",
    "        return random.sample(elements, k=n)\n",
    "    \n",
    "    import os\n",
    "import random\n",
    "\n",
    "def get_random_samples(directory, num_samples, seed=1234, replace=0):\n",
    "    \"\"\"\n",
    "    Get m random samples from n sub-directories of a directory, ensuring at least one sample is chosen from each sub-directory\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    subdirectories = [subdir for subdir in os.listdir(directory) if os.path.isdir(os.path.join(directory, subdir))]\n",
    "    num_subdirectories = len(subdirectories)\n",
    "    samples = []\n",
    "    \n",
    "    # Ensure at least one sample is chosen from each sub-directory\n",
    "    for subdir in subdirectories:\n",
    "        files = os.listdir(os.path.join(directory, subdir))\n",
    "        if files:\n",
    "            file = os.path.join(directory, subdir, random.choice(files))\n",
    "            samples.append(file)\n",
    "    \n",
    "    # Remove duplicates from the list of samples\n",
    "    samples = list(set(samples))\n",
    "    \n",
    "    # Choose remaining samples, allowing for replacement if necessary\n",
    "    while len(samples) < num_samples:\n",
    "        subdir = random.choice(subdirectories)\n",
    "        files = os.listdir(os.path.join(directory, subdir))\n",
    "        if files:\n",
    "            file = os.path.join(directory, subdir, random.choice(files))\n",
    "            if replace or file not in samples:\n",
    "                samples = list(set(samples))\n",
    "                samples.append(file)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6151ab-33c0-49f0-9649-6c90da1068e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_data(path, size, batch_size, class_mode, seed):\n",
    "    data = ImageDataGenerator(rescale=1/255)\n",
    "    return data.flow_from_directory(path,\n",
    "                                    target_size=(size, size),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode=class_mode,\n",
    "                                    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ad466-c86b-43f0-b671-584cbac5d67c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "    custom_model - Defines custom CNN model architecture\n",
    "    evaluate_models - Evaluates and compares multiple models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e0344-15aa-444c-a442-bec8ca470046",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1\n",
    "##### Note\n",
    "- data4 = Original sizes\n",
    "- data4b = 256\n",
    "- data4c = 512\n",
    "- data4d = 244\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def41ee5-dd0c-4d9f-a37c-3e58e1fdd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model(data_dir, batch_size=32, epochs=10, img_size=244, num_classes=9):\n",
    "    \n",
    "    # Build the model architecture\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(img_size, img_size, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdda6ac-79e2-4589-bd22-45d119c08cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData1').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU available, training on GPU...\")\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "    else:\n",
    "        print(\"GPU not available, training on CPU...\")\n",
    "        device_name = \"/CPU:0\"\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "    # Model creation\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "              ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "              ('VGG', VGG16, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    \n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        if model_fn == custom_model:\n",
    "            model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "        else:\n",
    "            base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "            x = Flatten()(base_model.output)\n",
    "            x = Dense(units=512, activation='relu')(x)\n",
    "            x = Dense(units=256, activation='relu')(x)\n",
    "            output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "            model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        # optimizer = Adam(learning_rate=0.001)\n",
    "        # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Move the model to the GPU if available\n",
    "        with tf.device(device_name):\n",
    "            # Define the learning rate schedule\n",
    "            def lr_schedule(epoch):\n",
    "                learning_rate = 0.0001\n",
    "                if epoch > 30:\n",
    "                    learning_rate *= 0.1\n",
    "                elif epoch > 20:\n",
    "                    learning_rate *= 0.01\n",
    "                print('Learning rate:', learning_rate)\n",
    "                print(get_nvidia_gpu_memory())\n",
    "                return learning_rate\n",
    "\n",
    "            # Define the callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model1.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            print(f'Training Model: {name}')\n",
    "            start_times = datetime.now()\n",
    "            print(f'{name} Started: {start_times}')\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=epochs,\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=len(val_generator),\n",
    "                callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "                )        \n",
    "            end_times = datetime.now()\n",
    "            print(f'{name} Ended: {end_times}')\n",
    "            print(f'Duration: {end_times - start_times}')\n",
    "            \n",
    "            # Run the garbage collector\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "            # save your model and its history to disk\n",
    "            model.save(f'{Folder}/wind_turbine_{name}_model1_{now}.h5')\n",
    "            with open(f'{Folder}/wind_turbine_{name}_history1_{now}.pkl', 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "     \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData1').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37a679-4290-49b0-a8ed-32cd53fc816d",
   "metadata": {},
   "source": [
    "### Models 1b\n",
    "- Use distributed training\n",
    "- multiple GPUs and want to take advantage of distributed training to potentially speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811b6cb0-26c0-447d-bf32-a5c94dd95f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models1b(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData1b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Determine device to use\n",
    "    devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif len(devices) == 1:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "    # Model creation\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "              ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "              ('VGG16', VGG16, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        with strategy.scope():\n",
    "            if model_fn == custom_model:\n",
    "                model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "            else:\n",
    "                base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "                x = Flatten()(base_model.output)\n",
    "                x = Dense(units=512, activation='relu')(x)\n",
    "                x = Dense(units=256, activation='relu')(x)\n",
    "                output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "                model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Define the learning rate schedule\n",
    "        def lr_schedule(epoch):\n",
    "            learning_rate = 0.0001\n",
    "            if epoch > 30:\n",
    "                learning_rate *= 0.1\n",
    "            elif epoch > 20:\n",
    "                learning_rate *= 0.01\n",
    "            print('Learning rate:', learning_rate)\n",
    "            print(get_nvidia_gpu_memory())\n",
    "            return learning_rate\n",
    "\n",
    "        # Define the callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model1b_{now}.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        # Train the model\n",
    "        print(f'Training Model: {name}')\n",
    "        start_times = datetime.now()\n",
    "        print(f'{name} Started: {start_times}')\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(train_generator),\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=len(val_generator),\n",
    "            callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "            )        \n",
    "        end_times = datetime.now()\n",
    "        print(f'{name} Ended: {end_times}')\n",
    "        print(f'Duration: {end_times - start_times}')\n",
    "        \n",
    "        # Run the garbage collector\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        # save your model and its history to disk\n",
    "        model.save(f'{Folder}/wind_turbine_{name}_model1b_{now}.h5')\n",
    "        with open(f'{Folder}/wind_turbine_{name}_history1b_{now}.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData1b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8cba2-5c3a-4748-a31d-d4946f98d66d",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9982177e-cb3f-42a1-b70d-1055267ca6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_modelB(data_dir, batch_size=32, epochs=10, img_size=244, num_classes=9):\n",
    "    \n",
    "    # Build the model architecture\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(img_size, img_size, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b40f5a36-bfc8-4248-8881-b5c0a9939e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modelsb(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData2').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU available, training on GPU...\")\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "    else:\n",
    "        print(\"GPU not available, training on CPU...\")\n",
    "        device_name = \"/CPU:0\"\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "    # Model creation\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "              ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "              ('VGG', VGG16, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    \n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        if model_fn == custom_model:\n",
    "            model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "        else:\n",
    "            base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "            x = Flatten()(base_model.output)\n",
    "            x = Dense(units=1024, activation='relu')(x)\n",
    "            x = Dense(units=512, activation='relu')(x)\n",
    "            output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "            model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Move the model to the GPU if available\n",
    "        with tf.device(device_name):\n",
    "            # Define the learning rate schedule\n",
    "            def lr_schedule(epoch):\n",
    "                learning_rate = 0.0001\n",
    "                if epoch > 30:\n",
    "                    learning_rate *= 0.1\n",
    "                elif epoch > 20:\n",
    "                    learning_rate *= 0.01\n",
    "                print('Learning rate:', learning_rate)\n",
    "                print(get_nvidia_gpu_memory())\n",
    "                return learning_rate\n",
    "\n",
    "            # Define the callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            # print('Training Model:',name)\n",
    "            print(f'Training Model: {name}')\n",
    "            start_times = datetime.now()\n",
    "            print(f'{name} Started: {start_times}')\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=epochs,\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=len(val_generator),\n",
    "                callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "                )        \n",
    "            end_times = datetime.now()\n",
    "            print(f'{name} Ended: {end_times}')\n",
    "            print(f'Duration: {end_times - start_times}')\n",
    "            \n",
    "            # Run the garbage collector\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "            # save your model and its history to disk\n",
    "            model.save(f'{Folder}/wind_turbine_{name}_model2_{now}.h5')\n",
    "            with open(f'{Folder}/wind_turbine_{name}_history2_{now}.pkl', 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData2').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125423f0-37e8-405b-a11b-b0cc52030933",
   "metadata": {},
   "source": [
    "### Models 2b\n",
    "- Use distributed training\n",
    "- multiple GPUs and want to take advantage of distributed training to potentially speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc8ebf60-ca28-48d2-bb53-772b0e990e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modelsb2(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData2b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Determine device to use\n",
    "    devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif len(devices) == 1:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "    # Model creation\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "              ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "              ('VGG16', VGG16, 'imagenet'), ('Custom', custom_modelB, None)]\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        with strategy.scope():\n",
    "            if model_fn == custom_modelB:\n",
    "                model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "            else: \n",
    "                base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "                x = Flatten()(base_model.output)\n",
    "                x = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "                x = Dropout(0.5)(x)\n",
    "                x = Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "                x = Dropout(0.5)(x)\n",
    "                output = Dense(units=train_generator.num_classes, activation='softmax')(x)\n",
    "                model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # Define the learning rate schedule\n",
    "        def lr_schedule(epoch):\n",
    "            learning_rate = 0.0001\n",
    "            if epoch > 30:\n",
    "                learning_rate *= 0.1\n",
    "            elif epoch > 20:\n",
    "                learning_rate *= 0.01\n",
    "            print('Learning rate:', learning_rate)\n",
    "            print(get_nvidia_gpu_memory())\n",
    "            return learning_rate\n",
    "\n",
    "        # Define the callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model2b_{now}.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        # Train the model\n",
    "        # print('Training Model:',name)\n",
    "        print(f'Training Model: {name}')\n",
    "        start_times = datetime.now()\n",
    "        print(f'{name} Started: {start_times}')\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(train_generator),\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=len(val_generator),\n",
    "            callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "            )        \n",
    "        end_times = datetime.now()\n",
    "        print(f'{name} Ended: {end_times}')\n",
    "        print(f'Duration: {end_times - start_times}')\n",
    "        \n",
    "        # Run the garbage collector\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        # save your model and its history to disk\n",
    "        model.save(f'{Folder}/wind_turbine_{name}_model2b_{now}.h5')\n",
    "        with open(f'{Folder}/wind_turbine_{name}_history2b_{now}.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData2b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380cfbc-88ce-410e-a4e9-55b8584cb66a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ff994-f77a-4415-996a-52551e1cd029",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model Training\n",
    "\n",
    "    generate_training_params - Generates training parameters\n",
    "    load_data_generator - Loads data generator\n",
    "    \n",
    "##### Hyperparameter Tuning\n",
    "    calculate_steps_per_epoch_and_epochs - Calculates training parameters\n",
    "    calculate_batch_size_steps_per_epoch_and_epochs - Calculates training parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75ae273a-41f8-4cc2-bf48-4ce7ba2f5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_params(num_train, num_val, num_test):\n",
    "    # Batch size\n",
    "    batch_size = 32 if num_train <= 10000 else 38\n",
    "    # batch_size = 32 if num_train <= 10000 else 64\n",
    "    \n",
    "    # Epochs\n",
    "    epochs = 25 if num_train <= 10000 else 28\n",
    "    # epochs = 25 if num_train <= 10000 else 50\n",
    "    \n",
    "    # Steps per epoch\n",
    "    steps_per_epoch = num_train // batch_size\n",
    "    \n",
    "    # Validation steps\n",
    "    validation_steps = num_val // batch_size\n",
    "    \n",
    "    return batch_size, epochs, steps_per_epoch, validation_steps\n",
    "\n",
    "def load_data_generator(datagen, data_dir, subdir, img_size = 244, batch_size = 32):\n",
    "    if subdir != 'test':\n",
    "        shuffles=True\n",
    "    else:\n",
    "        shuffles=False\n",
    "    return datagen.flow_from_directory(\n",
    "        os.path.join(data_dir, subdir),\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle = shuffles\n",
    "        # class_mode='categorical'\n",
    "    )\n",
    "\n",
    "        \n",
    "        \n",
    "def calculate_steps_per_epoch_and_epochs(num_samples, desired_epochs):\n",
    "    batch_size = 128  # example batch size\n",
    "    steps_per_epoch = num_samples // batch_size + 1\n",
    "    epochs = desired_epochs\n",
    "    while steps_per_epoch * epochs < num_samples:\n",
    "        epochs += 1\n",
    "    return steps_per_epoch, epochs\n",
    "\n",
    "def calculate_batch_size_steps_per_epoch_and_epochs(sample, desired_epochs):\n",
    "    num_samples = len(sample)\n",
    "    batch_size = train_generator.batch_size\n",
    "    steps_per_epoch = num_samples // batch_size + 1\n",
    "    epochs = desired_epochs\n",
    "    while steps_per_epoch * epochs < num_samples:\n",
    "        epochs += 1\n",
    "    return batch_size, steps_per_epoch, epochs\n",
    "\n",
    "def calculate_batch_size_steps_per_epoch_and_epochsb(train_generator, desired_epochs):\n",
    "    num_samples = train_generator.samples\n",
    "    batch_size = train_generator.batch_size\n",
    "    steps_per_epoch = num_samples // batch_size + 1\n",
    "    epochs = desired_epochs\n",
    "    while steps_per_epoch * epochs < num_samples:\n",
    "        epochs += 1\n",
    "    return batch_size, steps_per_epoch, epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d35a31-b2a8-451b-9785-15667189a3a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "    checkHistory - Checks if history object or dict\n",
    "    plot_training_history - Plots accuracy/loss history\n",
    "    plot_training_histories - plot accuracy/loss history for multiple models\n",
    "    EvaluateTest - Evaluates model on test set    \n",
    "    calculate_metrics_multiclass - \n",
    "    detect_faults - Detects faults in images with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c58f7aa-83fe-445a-8937-92cbd0ac9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHistory(history):\n",
    "    return history.history if 'history' in history else history\n",
    "\n",
    "def plot_training_history(history, output = 'results'):\n",
    "\n",
    "    CreateDir(output)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax1.plot(history['accuracy'])\n",
    "    ax1.plot(history['val_accuracy'])\n",
    "    ax1.set_title('Model accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    ax2.plot(history['loss'])\n",
    "    ax2.plot(history['val_loss'])\n",
    "    ax2.set_title('Model loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.savefig(os.path.join(output,'training_history.png'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_training_historyB(history, output = 'results'):\n",
    "\n",
    "    CreateDir(output)\n",
    "    plt.plot(history[\"accuracy\"])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "\n",
    "    plt.savefig(os.path.join(output,'training_historyB.png'), bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d2dba-6ee8-4f98-8263-9645bf8552a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(filtered_list):\n",
    "    model_list = []\n",
    "    \n",
    "    for file_path in filtered_list:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            model_list.append(model)\n",
    "    \n",
    "    return model_list\n",
    "\n",
    "def get_model_names(filtered_list):\n",
    "    model_names = []\n",
    "\n",
    "    for f in filtered_list:\n",
    "        name = os.path.basename(f)\n",
    "        name, _ = os.path.splitext(name)\n",
    "\n",
    "        date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{4})', name)\n",
    "        if date_match:\n",
    "            date = date_match.group(1)\n",
    "            name = name.replace(f'_{date}', '')\n",
    "\n",
    "        name = name.replace('wind_turbine_', '').replace('_history_', '').replace('_history1', '').replace('_','-')\n",
    "        model_names.append(name)\n",
    "\n",
    "    return model_names\n",
    "\n",
    "def plot_training_histories(histories, model_names, output='results'):\n",
    "\n",
    "    CreateDir(output)\n",
    "    \n",
    "    metrics = ['accuracy', 'val_accuracy', 'loss', 'val_loss']\n",
    "    titles = ['Train Accuracy', 'Validation Accuracy', 'Train Loss', 'Validation Loss']\n",
    "    ylabels = ['Accuracy', 'Accuracy', 'Loss', 'Loss']\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].set_ylabel(ylabels[i])\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "\n",
    "        for history in histories:\n",
    "            if 'history' in history:\n",
    "                history = history['history']\n",
    "            axs[i].plot(history[metric])\n",
    "\n",
    "        axs[i].margins(x=0)  # Remove horizontal margins to make full use of the plot\n",
    "\n",
    "        # Get the maximum number of epochs from all histories\n",
    "        max_epochs = max(len(history[metric]) for history in histories)\n",
    "\n",
    "        # Calculate the number of intervals on the x-axis\n",
    "        num_intervals = 5  # Adjust this value to control the number of intervals\n",
    "        interval = max_epochs // num_intervals\n",
    "\n",
    "        # Set the x-axis tick positions and labels\n",
    "        x_ticks = range(0, max_epochs + 1, interval)\n",
    "        x_labels = [str(x) for x in x_ticks]\n",
    "        axs[i].set_xticks(x_ticks)\n",
    "        axs[i].set_xticklabels(x_labels)\n",
    "        \n",
    "        axs[i].set_xlabel('Epoch')\n",
    "\n",
    "    # Create a single legend outside the subplots with additional spacing and a title\n",
    "    fig.legend(model_names, loc='upper right', bbox_to_anchor=(1.16, 0.95), title='Legend')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output, 'training_histories.png'), bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e8ec707-93ac-43e9-9a5b-fc9dbe138355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateTest(data_dir, model, history, batch_size=32, img_size=256,output = 'results'):\n",
    "\n",
    "    CreateDir(output)\n",
    "\n",
    "    # Data augmentation for test set\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)    \n",
    "    # load_data_generator(datagen, data_dir, subdir, img_size = 244, batch_size = 32)\n",
    "    test_generator = load_data_generator(test_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "        \n",
    "    \n",
    "    # Create an empty dictionary to store time per image\n",
    "    time_per_image = {}\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    # print('\\n Evaluate the model on the test set')\n",
    "    start_time = time.time()\n",
    "    scores = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
    "    end_time = time.time()\n",
    "    # print(\"Test loss:\", scores[0])\n",
    "    # print(\"Test accuracy:\", scores[1])\n",
    "\n",
    "    # Calculate time per image in seconds\n",
    "    time_taken = end_time - start_time\n",
    "    time_per_image['test'] = time_taken / len(test_generator.filenames)\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_per_image, orient='index', columns=['Time per image (s)'])\n",
    "    print('EvaluateTime_report')\n",
    "    print(df)    \n",
    "    df.to_csv(os.path.join(output,\"EvaluateTime_report.csv\"), index=True)\n",
    "    \n",
    "    \n",
    "    history = checkHistory(history)\n",
    "    \n",
    "    # get the index of the highest accuracy value\n",
    "    print('Training indexes')\n",
    "    highest_acc_index = np.argmax(history['val_accuracy'])\n",
    "    \n",
    "    # create a dictionary with the values\n",
    "    data = {'first': [history['loss'][0], history['accuracy'][0], \n",
    "                     history['val_loss'][0], history['val_accuracy'][0]],\n",
    "            'lowest': [min(history['loss']), min(history['accuracy']), \n",
    "                        min(history['val_loss']), min(history['val_accuracy'])],\n",
    "            'highest': [max(history['loss']), max(history['accuracy']), \n",
    "                        max(history['val_loss']), max(history['val_accuracy'])],\n",
    "            'last': [history['loss'][-1], history['accuracy'][-1], \n",
    "                     history['val_loss'][-1], history['val_accuracy'][-1]]\n",
    "           }\n",
    "    \n",
    "        # create a dataframe with the values as the index\n",
    "    df = pd.DataFrame(data, index=['training loss', 'training accuracy', 'validation loss', 'validation accuracy'])\n",
    "    # convert values to 4 decimal places\n",
    "    df = df.round(4)\n",
    "    # df = df.apply(lambda x: round(x, 4) if isinstance(x[1], float) else x)\n",
    "    # save the dataframe as CSV\n",
    "    # df.to_csv(os.path.join(output_dir, 'history.csv'), index=True)\n",
    "    df.to_csv(os.path.join(output,'history.csv'), index=True)\n",
    "    # print the dataframe\n",
    "    print(df,'\\n')\n",
    "\n",
    "    plot_training_history(history)\n",
    "\n",
    "    plot_training_historyB(history)\n",
    "    \n",
    "    classNmaes = list(test_generator.class_indices.keys())\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    print('\\n Evaluate the model on the test set')\n",
    "    # scores = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
    "    print(\"Test loss:\", scores[0])\n",
    "    print(\"Test accuracy:\", scores[1])\n",
    "    \n",
    "    # Get the true labels and the predicted labels from the test set\n",
    "    true_labels = test_generator.classes\n",
    "    predicted_labels = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "    # Compute the binary true labels and predicted labels for each class\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "    binary_true_labels = []\n",
    "    binary_predicted_labels = []\n",
    "    for i in range(len(class_names)):\n",
    "        class_idx = test_generator.class_indices[class_names[i]]\n",
    "        binary_true_labels.append((true_labels == class_idx).astype(int))\n",
    "        binary_predicted_labels.append(predicted_labels[:, i])\n",
    "\n",
    "    # Compute the ROC curve and AUC for each class\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    auc_roc_scores = []\n",
    "    for i in range(len(class_names)):\n",
    "        fpr, tpr, thresholds = roc_curve(binary_true_labels[i], binary_predicted_labels[i])\n",
    "        roc_auc = roc_auc_score(binary_true_labels[i], binary_predicted_labels[i])\n",
    "        auc_roc_scores.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=2, label=class_names[i] + ' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver operating characteristic ROC', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\")    \n",
    "    plt.savefig(os.path.join(output,'auc_roc_scores.png'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"clases: \\n\", class_names)\n",
    "    # Compute the classification report    \n",
    "    predicted_labels = np.argmax(predicted_labels, axis=1)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)    \n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    df_cm = df_cm.round(2)\n",
    "    df_cm.to_csv(os.path.join(output,'confusion_matrix.csv'), index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cr = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
    "    cr_dict = classification_report(true_labels, predicted_labels, target_names=class_names, output_dict=True)\n",
    "    df_cr = pd.DataFrame(cr_dict).transpose()\n",
    "    df_cr = df_cr.round(2)\n",
    "    df_cr.to_csv(os.path.join(output,\"classification_report.csv\"), index=True)\n",
    "    \n",
    "    print(\" \\nConfusion matrix:\\n\", cm)\n",
    "    print(df_cm)\n",
    "    \n",
    "    \n",
    "    # plot_confusion_matrix(cm, class_names)\n",
    "    plot_confusion_matrixN(cm, class_names,normalize=True)\n",
    "    \n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    # plt.ylabel('True labels')    \n",
    "    plt.ylabel('Ground Truth label') \n",
    "    name = \"Confusion_matrix_normalized.png\"\n",
    "    plt.savefig(os.path.join(output,name))\n",
    "    # plt.show()\n",
    "    \n",
    "    print(\" \\nClassification report:\\n\", cr)\n",
    "    print(df_cr)\n",
    "    \n",
    "\n",
    "    # Create a table of AUC-ROC scores for each class\n",
    "    auc_roc_table = pd.DataFrame({'Class': class_names, 'AUC-ROC': auc_roc_scores})\n",
    "    auc_roc_table = auc_roc_table.round(2)    \n",
    "    auc_roc_table.to_csv(os.path.join(output,\"auc_roc_table.csv\"), index=False)\n",
    "    print(\"\\nauc_roc_table:\\n\", auc_roc_table)\n",
    "    \n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    true_labels_binary = lb.fit_transform(true_labels)\n",
    "    predicted_labels_binary = lb.transform(predicted_labels)\n",
    "\n",
    "    avg_precision = average_precision_score(true_labels_binary, predicted_labels_binary, average='macro')\n",
    "    print(\"Average Precision Score: \", avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae63c9d-b643-40d8-96ac-4b5351e8d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics_multiclass(y_true, y_pred, class_names):\n",
    "    metrics = {}\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Create binary label arrays\n",
    "        y_true_class = (y_true == i).astype(int)\n",
    "        y_pred_class = (y_pred == i).astype(int)\n",
    "\n",
    "        # Calculate TP, FP, TN, FN\n",
    "        tp = np.sum(y_true_class * y_pred_class)\n",
    "        fp = np.sum((1 - y_true_class) * y_pred_class)\n",
    "        tn = np.sum((1 - y_true_class) * (1 - y_pred_class))\n",
    "        fn = np.sum(y_true_class * (1 - y_pred_class))\n",
    "\n",
    "        # Calculate TPR and FPR\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + tn)\n",
    "\n",
    "        # Save metrics for the class\n",
    "        metrics[class_name] = {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn, 'TPR': tpr, 'FPR': fpr}\n",
    "\n",
    "    # Convert metrics to DataFrame\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df = df.T\n",
    "    df.index.name = 'Class'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418c0a3-afe1-4696-8f87-d0214278a31f",
   "metadata": {},
   "source": [
    "### Model Inference/Prediction\n",
    "\n",
    "    detect_faults - Detects faults in images\n",
    "    detect_faultsdf - Detects faults and outputs dataframe\n",
    "    detect_faultsdfTrue - Detects faults without visualization\n",
    "    detect_faultsdfalone - Detects faults without grid visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42f1f736-aa42-45e7-8952-38b737076c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faults(model, image_path, class_names, img_size=256,threshold=0.2, output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: List of tuples representing the fault predictions\n",
    "        (fault_name, x1, y1, x2, y2, confidence)\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                x1, y1, x2, y2 = (10 * i, 10 * i, 200 - 10 * i, 200 - 10 * i)\n",
    "                colord = colors[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1, f'{class_names[i]} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "\n",
    "        ax.set_title(path.split('\\\\')[-2])\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        predictions = [(class_names[i], x1, y1, x2, y2, prediction)\n",
    "                       for i, prediction in enumerate(predictions[0])\n",
    "                       if prediction > 0.5]\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49d07c72-2c54-4d9f-965f-8d2868fc4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faultsdf(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: List of tuples representing the fault predictions\n",
    "        (fault_name, x1, y1, x2, y2, confidence)\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    rows = 3 if len(image_paths) < 7 else 4\n",
    "    numm = math.ceil(len(image_paths) / rows)\n",
    "    # dim = (256, 256)\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    gs1 = gridspec.GridSpec(numm, rows)\n",
    "    gs1.update(wspace=0.025, hspace=0.08)\n",
    "\n",
    "    all_predictions = []\n",
    "    for idx, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "        pred_labels = []\n",
    "        confidences = []        \n",
    "\n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        ax1 = plt.subplot(gs1[idx])\n",
    "        ax1.imshow(np.squeeze(image))\n",
    "\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[i])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "                x1, y1, x2, y2 = (10 * i, 10 * i, 200 - 10 * i, 200 - 10 * i)\n",
    "                colord = colors[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax1.add_patch(rect)\n",
    "                ax1.text(x1, y1, f'{class_names[i]} ({prediction:.2f})',\n",
    "                         fontsize=14, fontweight='bold', color=colord)\n",
    "\n",
    "\n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "        \n",
    "        ax1.set_title(path.split(os.path.sep)[-2])\n",
    "        ax1.axis('off')\n",
    "\n",
    "        predictions = [(class_names[i], x1, y1, x2, y2, prediction)\n",
    "                       for i, prediction in enumerate(predictions[0])\n",
    "                       if prediction > 0.5]\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "        # all_predictions.append(predictions)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fa43bfb-5c36-46b1-9fe9-96ed524ded90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faultsdf(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size to resize to (default 256)\n",
    "    - threshold: Prediction threshold (default 0.2)\n",
    "    - n_cols: Number of columns in the final image grid (default 3)\n",
    "\n",
    "    Returns:\n",
    "    - df: Pandas dataframe with columns 'Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "    \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "    n_cols = 3 if len(image_paths) < 7 else 4\n",
    "\n",
    "    all_predictions = []\n",
    "    predicted_images = []\n",
    "    n_rows = int(np.ceil(len(image_paths) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "\n",
    "    colors = generate_colors(len(class_names))\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "                x1, y1, x2, y2 = (10 * j, 10 * j, 200 - 10 * j, 200 - 10 * j)\n",
    "                colord = colors[j]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1, f'{class_names[j]} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        ax.set_title(path.split(sepps)[-2])\n",
    "        ax.axis('off')\n",
    "\n",
    "        predicted_images.append(fig)\n",
    "        \n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "            \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "\n",
    "    for i in range(len(image_paths), n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.005, hspace=0.15)  # modify the spacing between subplots\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output,'final_image.png'), bbox_inches='tight', pad_inches=0)\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "    \n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(os.path.join(output,'SamplePredicted.csv'), index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c526532-cc6b-42ee-a468-e5ba39e60987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_faultsdfTrue(model, image_path, class_names, img_size=256, threshold=0.2,output='results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size to resize to (default 256)\n",
    "    - threshold: Prediction threshold (default 0.2)\n",
    "\n",
    "    Returns:\n",
    "    - df: Pandas dataframe with columns 'Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'\n",
    "    \"\"\"\n",
    "    \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "            \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(os.path.join(output,'SamplePredicted.csv'), index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "362c5bd2-052c-4bc2-9db0-694fe51dcf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_faultsdf(model, image_path, class_names, img_size=256, threshold=0.2,output = 'results'):  \n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size to resize to (default 256)\n",
    "    - threshold: Prediction threshold (default 0.2)\n",
    "    - n_cols: Number of columns in the final image grid (default 3)\n",
    "\n",
    "    Returns:\n",
    "    - df: Pandas dataframe with columns 'Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'\n",
    "    \"\"\"  \n",
    "\n",
    "    CreateDir(output)\n",
    "    \n",
    "    sepps = os.path.sep \n",
    "    pattern = r'[\\[\\](){}_\\s]'\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "    n_cols = 3 if len(image_paths) < 7 else 4\n",
    "\n",
    "    all_predictions = []\n",
    "    predicted_images = []\n",
    "    n_rows = int(np.ceil(len(image_paths) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "\n",
    "    colors = generate_colors(len(class_names))\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "                x1, y1, x2, y2 = (10 * j, 10 * j, 200 - 10 * j, 200 - 10 * j)\n",
    "                colord = colors[j]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1, f'{class_names[j]} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        ax.set_title(path.split(sepps)[-2])\n",
    "        ax.axis('off')\n",
    "\n",
    "        predicted_images.append(fig)\n",
    "        \n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence, prediction_time_in_sec))\n",
    "            \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "    df['Image'] = df['Image'].str.replace(pattern, '')    \n",
    "    df['Image'] = df['Image'].str.replace('[\\(\\)\\[\\]\\{\\}]', '')\n",
    "    df['Image'] = df['Image'].str[-9:]\n",
    "    df['Image'] = df['Image'].str.capitalize()\n",
    "\n",
    "    for i in range(len(image_paths), n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.005, hspace=0.15)  # modify the spacing between subplots\n",
    "    plt.savefig(os.path.join(output,'final_image.png'), bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    # count number of rows with one predicted label\n",
    "    one_label = (df['Predicted Label'].str.count(',') == 0).sum()\n",
    "    # count number of rows with more than one predicted label\n",
    "    more_labels = (df['Predicted Label'].str.count(',') > 0).sum()\n",
    "    \n",
    "    total_Images = df.shape[0]\n",
    "    total_Predictions = df_split.shape[0]\n",
    "    # Assuming the variables are already defined\n",
    "    print(f\"\\n\\n\\nNumber of rows with one predicted label: {one_label}\")\n",
    "    print(f\"Number of rows with more than one predicted label: {more_labels}\")\n",
    "    print(f\"Total Images label: {total_Images}\")\n",
    "    print(f\"Total predicted label: {total_Predictions}\")\n",
    "\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Calculate percentage of accuracy and wrong predictions\n",
    "    accuracy = (count / total_Predictions) * 100\n",
    "    wrong_predictions = (countb / total_Predictions) * 100\n",
    "\n",
    "    print(f\"Percentage of accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Percentage of wrong predictions: {wrong_predictions:.2f}%\\n\\n\")\n",
    "    \n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(os.path.join(output,'SamplePredicted.csv'), index=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95644465-8e5f-4ca8-abfa-3b59a1342073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_faultsdfalone(model, image_path, class_names, img_size=256, threshold=0.2,output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size to resize to (default 256)\n",
    "    - threshold: Prediction threshold (default 0.2)\n",
    "\n",
    "    Returns:\n",
    "    - df: Pandas dataframe with columns 'Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "    \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "        \n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence,prediction_time_in_sec))\n",
    "            \n",
    "    # df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])    \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    # count number of rows with one predicted label\n",
    "    one_label = (df['Predicted Label'].str.count(',') == 0).sum()\n",
    "    # count number of rows with more than one predicted label\n",
    "    more_labels = (df['Predicted Label'].str.count(',') > 0).sum()\n",
    "    \n",
    "    total_Images = df.shape[0]\n",
    "    total_Predictions = df_split.shape[0]\n",
    "    # Assuming the variables are already defined\n",
    "    print(f\"\\n\\n\\nNumber of rows with one predicted label: {one_label}\")\n",
    "    print(f\"Number of rows with more than one predicted label: {more_labels}\")\n",
    "    print(f\"Total Images label: {total_Images}\")\n",
    "    print(f\"Total predicted label: {total_Predictions}\")\n",
    "\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Calculate percentage of accuracy and wrong predictions\n",
    "    accuracy = (count / total_Predictions) * 100\n",
    "    wrong_predictions = (countb / total_Predictions) * 100\n",
    "\n",
    "    print(f\"Percentage of accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Percentage of wrong predictions: {wrong_predictions:.2f}%\\n\\n\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(os.path.join(output,'SamplePredicted.csv'), index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8749028d-cdb1-415f-9b50-80643d063d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faultsdfaloneb(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):  \n",
    "\n",
    "    CreateDir(output)  \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "        \n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence,prediction_time_in_sec))\n",
    "            \n",
    "    # df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])    \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "\n",
    "    # Calculate TP, FP, TN, FN for each image\n",
    "    for image_name, df_image in df_split.groupby('Image'):\n",
    "        gt_labels = set(df_image['Ground_Truth_Label'].values)\n",
    "        pred_labels = set(df_image['Predicted_Label'].values)\n",
    "        TP = len(gt_labels.intersection(pred_labels))\n",
    "        FP = len(pred_labels - gt_labels)\n",
    "        TN = 0\n",
    "        FN = len(gt_labels - pred_labels)\n",
    "        df.loc[df['Image'] == image_name, 'TP'] = TP\n",
    "        df.loc[df['Image'] == image_name, 'FP'] = FP\n",
    "        df.loc[df['Image'] == image_name, 'TN'] = TN\n",
    "        df.loc[df['Image'] == image_name, 'FN'] = FN\n",
    "\n",
    "    # Calculate TPR and FPR\n",
    "    TP = df['TP'].sum()\n",
    "    FP = df['FP'].sum()\n",
    "    TN = df['TN'].sum()\n",
    "    FN = df['FN'].sum()\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "\n",
    "    print(f\"TPR: {TPR:.2f}\")\n",
    "    print(f\"FPR: {FPR:.2f}\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(os.path.join(output,'SamplePredictedb.csv'), index=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62cdb714-ceb6-4740-b2ac-99c9067809a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faults_images(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):\n",
    "\n",
    "    CreateDir(output)\n",
    "    sepps = os.path.sep\n",
    "\n",
    "    image_paths = get_image_paths(image_path)\n",
    "    n_cols = min(4, len(image_paths))\n",
    "\n",
    "    all_predictions = []\n",
    "    data = []\n",
    "\n",
    "    fig, axes = plt.subplots(len(image_paths), n_cols, figsize=(4 * n_cols, 4 * len(image_paths)))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        ax = axes[i*n_cols:(i+1)*n_cols]\n",
    "        ax[0].imshow(np.squeeze(image))\n",
    "        ax[0].set_title(path.split(sepps)[-2])\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "                x1, y1, x2, y2 = (10 * j, 10 * j, 200 - 10 * j, 200 - 10 * j)\n",
    "                colord = colors[j]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax[0].add_patch(rect)\n",
    "                ax[0].text(x1, y1, f'{class_names[j]} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "\n",
    "        gt_label = path.split(sepps)[-2]\n",
    "        pred_label = ','.join(pred_labels) or 'None'\n",
    "        confidence = ','.join(f'{c:.2f}' for c in confidences) or 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction > threshold:\n",
    "                ax[j].imshow(np.squeeze(image))\n",
    "                ax[j].set_title(f'{class_names[j]} ({prediction:.2f})')\n",
    "                ax[j].axis('off')\n",
    "\n",
    "                predictions = [(class_names[j], x1, y1, x2, y2, prediction)\n",
    "                               for j, prediction in enumerate(predictions[0])\n",
    "                               if prediction > threshold]\n",
    "                for prediction in predictions:\n",
    "                    data.append([os.path.basename(path), gt_label, prediction[0], prediction[5]])\n",
    "\n",
    "        for k in range(j+1, n_cols):\n",
    "            ax[k].axis('off')\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.005)\n",
    "    # plt.show()\n",
    "    plt.savefig(os.path.join(output,'final_image.png'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    df_split = df.assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = (df_split['Ground Truth Label'] == df_split['Predicted_Label']).sum()\n",
    "    countb = (df_split['Ground Truth Label'] != df_split['Predicted_Label']).sum()\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c25f120b-ac5f-4ff7-a42e-71743020d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_faults1(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: Pandas dataframe with columns ['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence']\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[i])\n",
    "                confidences.append(prediction)\n",
    "\n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def detect_faults2(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: List of tuples representing the fault predictions\n",
    "        (fault_name, x1, y1, x2, y2, confidence)\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        predicted_labels = []\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                x1, y1, x2, y2 = (10 * i, 10 * i, 200 - 10 * i, 200 - 10 * i)\n",
    "                colord = colors[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                label = f'{class_names[i]} ({prediction:.2f})'\n",
    "                ax.text(x1, y1, label,\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "                predicted_labels.append((class_names[i], prediction))\n",
    "\n",
    "        ax.set_title(path.split('\\\\')[-2])\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        true_label = path.split('\\\\')[-2]\n",
    "        predictions_df = pd.DataFrame(predicted_labels, columns=['Predicted Label', 'Confidence'])\n",
    "        predictions_df['Image'] = os.path.basename(path)\n",
    "        predictions_df['Ground Truth Label'] = true_label\n",
    "        predictions_df = predictions_df[['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence']]\n",
    "        all_predictions.append(predictions_df)\n",
    "\n",
    "    all_predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "    # print(all_predictions_df)\n",
    "    return all_predictions_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_faults3(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions_df: pandas DataFrame representing the fault predictions for all images\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        predicted_labels = []\n",
    "        confidences = []\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                x1, y1, x2, y2 = (10 * i, 10 * i, 200 - 10 * i, 200 - 10 * i)\n",
    "                colord = colors[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                label = class_names[i]\n",
    "                predicted_labels.append(label)\n",
    "                confidences.append(prediction)\n",
    "                ax.text(x1, y1, f'{label} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "\n",
    "        ax.set_title(path.split('\\\\')[-2])\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # create a list of dictionaries for each predicted label and confidence\n",
    "        predictions_dict_list = []\n",
    "        for label, confidence in zip(predicted_labels, confidences):\n",
    "            predictions_dict_list.append({\n",
    "                'Image': path.split('\\\\')[-1],\n",
    "                'Ground Truth Label': path.split('\\\\')[-2],\n",
    "                'Predicted Label': label,\n",
    "                'Confidence': confidence\n",
    "            })\n",
    "\n",
    "        # create a DataFrame from the list of dictionaries\n",
    "        predictions_df = pd.DataFrame(predictions_dict_list)\n",
    "\n",
    "        all_predictions.append(predictions_df)\n",
    "\n",
    "    # concatenate all the predictions DataFrames into one\n",
    "    predictions_df = pd.concat(all_predictions)\n",
    "\n",
    "    # reset the index of the DataFrame\n",
    "    predictions_df = predictions_df.reset_index(drop=True)\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_faults4(model, image_path, class_names, img_size=256, threshold=0.2, output = 'results'):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size (default=256)\n",
    "    - threshold: Confidence threshold for predictions (default=0.2)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: DataFrame representing the fault predictions\n",
    "        (Image, Ground Truth Label, Predicted Label, Confidence)\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        predicted_classes = [class_names[i] for i, p in enumerate(predictions[0]) if p >= threshold]\n",
    "        # predicted_confidences = [p for i, p in enumerate(predictions[0]) if p >= threshold]\n",
    "        predicted_confidences = [round(p, 2) for i, p in enumerate(predictions[0]) if p >= threshold]\n",
    "\n",
    "\n",
    "        true_class = path.split(os.path.sep)[-2]\n",
    "        image_name = path.split(os.path.sep)[-1]\n",
    "\n",
    "        predictions = [(image_name, true_class, predicted_class, confidence)\n",
    "                       for predicted_class, confidence in zip(predicted_classes, predicted_confidences)]\n",
    "        \n",
    "        \n",
    "\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "\n",
    "    # print(predictions_df)\n",
    "    \n",
    "    count = len(df[df['Ground Truth Label'] == df['Predicted Label']])\n",
    "    count1 = len(df[df['Ground Truth Label'] != df['Predicted Label']])\n",
    "    print('\\nCorrect predictions:',count,'\\nWrong predictions:',count1,'\\n')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def ossep():\n",
    "    return os.path.sep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39298c-adcc-4cb8-b422-7acd7a9ddff2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "\n",
    "    generate_colors - Generates distinct colors\n",
    "    plot_image_grid - Plots a grid of images\n",
    "    plot_confusion_matrix - Plots confusion matrix\n",
    "    predictBar - Plots class probabilities as bars\n",
    "    predictBarColor - Plots class probabilities as colored bars\n",
    "    plot_probability_distribution_and_roc_curves_one_vs_rest - Plots probability distributions and ROC curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c176d27-548a-4581-9ae7-3434c74c77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colors(n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    # Define major colors in order of perceived visibility\n",
    "    major_colors = [\"blue\", \"orange\",\"black\",\"red\", \"green\", \"yellow\", \"purple\", \"pink\"]\n",
    "    # Convert major colors to HTML color codes and RGB values\n",
    "    major_colors_hex_rgb = [(webcolors.name_to_hex(color), webcolors.name_to_rgb(color)) for color in major_colors]\n",
    "    # Generate colors\n",
    "    hues = [random.uniform(0, 1) for _ in range(n - len(major_colors))]\n",
    "    saturations = [random.uniform(0.5, 1) for _ in range(n - len(major_colors))]\n",
    "    lightnesses = [random.uniform(0.4, 0.8) for _ in range(n - len(major_colors))]\n",
    "    hue_offsets = [random.choice([-0.2, 0, 0.2]) for _ in range(n - len(major_colors))]\n",
    "    colors_hls = [(hues[i]+hue_offsets[i], saturations[i], lightnesses[i]) for i in range(n - len(major_colors))]\n",
    "    colors_rgb = [tuple(round(c*255) for c in colorsys.hls_to_rgb(*hls)) for hls in colors_hls]\n",
    "    # Sort colors by proximity to the most distinguishable and visible colors\n",
    "    colors_hex_rgb = sorted(major_colors_hex_rgb + list(zip(map(webcolors.rgb_to_hex, colors_rgb), colors_rgb)), key=lambda c: min([abs(colorsys.rgb_to_hls(*c[1])[0] - colorsys.rgb_to_hls(*webcolors.name_to_rgb(color))[0]) for color in major_colors]))\n",
    "    # Convert RGB values to HTML color codes and return the first n colors\n",
    "    colors_hex = [c[0] for c in colors_hex_rgb]\n",
    "    return colors_hex[:n]\n",
    "\n",
    "\n",
    "def plot_image_grid(imglist):\n",
    "    rows = 3 if len(imglist) < 7 else 4\n",
    "    numm = math.ceil(len(imglist) / rows)\n",
    "    dim = (256, 256)\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    gs1 = gridspec.GridSpec(numm, rows)\n",
    "    gs1.update(wspace=0.025, hspace=0.08)\n",
    "\n",
    "    for idx, img_path in enumerate(imglist):\n",
    "        shp = Image.open(img_path).size\n",
    "        title = f'Image{idx} {shp}'\n",
    "        img = Image.open(img_path).resize(dim)\n",
    "        ax1 = plt.subplot(gs1[idx])\n",
    "        plt.title(title)\n",
    "        plt.grid(False)\n",
    "        plt.axis('on')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_aspect('equal')\n",
    "        plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77ab63cf-6512-45ce-8004-a9805b219fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get better visual of the confusion matrix:\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "             normalize=False,\n",
    "             title='Confusion matrix',\n",
    "             cmap=plt.cm.Blues, output = 'results'):\n",
    "    #Add Normalization Option\n",
    "    '''prints pretty confusion metric with normalization option '''\n",
    "\n",
    "    CreateDir(output)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "#     print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "#To get better visual of the confusion matrix:\n",
    "def plot_confusion_matrixN(cm, classes,\n",
    "             normalize=False,\n",
    "             title='Confusion matrix',\n",
    "             cmap=plt.cm.Blues, output = 'results'):\n",
    "    #Add Normalization Option\n",
    "    '''prints pretty confusion metric with normalization option '''    \n",
    "\n",
    "    CreateDir(output)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        name = \"Normalized confusion matrix.png\"\n",
    "        print(\"Normalized_confusion_matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        name = \"Confusion_matrix_without_normalization.png\"\n",
    "    \n",
    "#     print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(os.path.join(output,name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5548d55-b345-484d-8722-8b546ed6e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictBar(model, image_path, classes, img_size=244, num_images=None, output = 'results'):\n",
    "    \"\"\"\n",
    "    Predicts the class probabilities for a set of images using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model (tensorflow.keras.Model): The model to use for prediction.\n",
    "        image_path (str): The path to the directory/list containing the images to predict.\n",
    "        classes (list of str): A list of the class names in the order predicted by the model.\n",
    "        img_size (int, optional): The target size to resize the images. Defaults to 244.\n",
    "        num_images (int, optional): The number of images to plot the predictions for.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "    \n",
    "    imglist = get_image_paths(image_path)\n",
    "    num_images = num_images or len(imglist)\n",
    "    images = [img_to_array(load_img(img_path, target_size=(img_size, img_size))) / 255.0 for img_path in imglist[:num_images]]\n",
    "\n",
    "    # Predict the classes for the images\n",
    "    predictions = model.predict(np.array(images))\n",
    "\n",
    "    # Plot the images and predictions\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(16, 4 * num_images))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=-0.2)\n",
    "\n",
    "    for n in range(num_images):\n",
    "        # Plot the image\n",
    "        axes[n, 0].imshow(load_img(imglist[n], target_size=(299, 299)))\n",
    "        axes[n, 0].axis('off')\n",
    "        axes[n, 0].set_title(imglist[n].split(os.path.sep)[-2])\n",
    "\n",
    "        # Plot the predictions\n",
    "        bar_plot = axes[n, 1].bar(range(len(classes)), predictions[n])\n",
    "        axes[n, 1].set_xticks(range(len(classes)))\n",
    "        axes[n, 1].set_xticklabels(classes, rotation=15, ha=\"right\")\n",
    "        axes[n, 1].set_title(f\"Model prediction: {classes[np.argmax(predictions[n])]}\")\n",
    "\n",
    "        # Add text labels for predicted probabilities\n",
    "        for i, v in enumerate(predictions[n]):\n",
    "            if v > 0.004:\n",
    "                axes[n, 1].text(i, v, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "    plt.savefig(os.path.join(output,'predictedBars.png'), bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def predictBarColor(model, image_path, classes, img_size=244, num_images=None, output = 'results'):\n",
    "    \"\"\"\n",
    "    Predicts the class probabilities for a set of images using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model (tensorflow.keras.Model): The model to use for prediction.\n",
    "        image_path (str): The path to the directory/list containing the images to predict.\n",
    "        classes (list of str): A list of the class names in the order predicted by the model.\n",
    "        img_size (int, optional): The target size to resize the images. Defaults to 244.\n",
    "        num_images (int, optional): The number of images to plot the predictions for.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    CreateDir(output)\n",
    "    imglist = get_image_paths(image_path)\n",
    "    num_images = num_images or len(imglist)\n",
    "    images = [img_to_array(load_img(img_path, target_size=(img_size, img_size))) / 255.0 for img_path in imglist[:num_images]]\n",
    "\n",
    "    # Predict the classes for the images\n",
    "    predictions = model.predict(np.array(images))\n",
    "\n",
    "    # Define colors for each class\n",
    "    num_classes = len(classes)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, num_classes))\n",
    "\n",
    "    # Plot the images and predictions\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(16, 4 * num_images))\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=-0.2)\n",
    "    # fig.subplots_adjust(hspace=0.6, wspace=-0.2)\n",
    "\n",
    "    for n in range(num_images):\n",
    "        # Plot the image\n",
    "        axes[n, 0].imshow(load_img(imglist[n], target_size=(299, 299)))\n",
    "        axes[n, 0].axis('off')\n",
    "        axes[n, 0].set_title(imglist[n].split(os.path.sep)[-2])\n",
    "      \n",
    "        # Plot the predictions        \n",
    "        bar_plot = axes[n, 1].bar(range(len(classes)), predictions[n], color=colors)\n",
    "        axes[n, 1].set_xticks(range(len(classes)))\n",
    "        axes[n, 1].set_xticklabels(range(len(classes)))\n",
    "        # axes[n, 1].set_xticklabels(classes, rotation=20, ha=\"right\")\n",
    "        axes[n, 1].set_title(f\"Model prediction: {classes[np.argmax(predictions[n])]}\")\n",
    "\n",
    "        # Add text labels for predicted probabilities\n",
    "        for i, v in enumerate(predictions[n]):\n",
    "            if v > 0.004:\n",
    "                axes[n, 1].text(i, v, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "        # Create legend with class names and corresponding colors\n",
    "        legend_handles = [mpatches.Patch(color=colors[i], label=classes[i]) for i in range(len(classes))]\n",
    "        axes[n, 1].legend(handles=legend_handles, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "    plt.savefig(os.path.join(output,'predictedBarsColor.png'), bbox_inches='tight')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b665dea1-d0d4-473d-bd3b-46d4bb0d319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probability_distribution_and_roc_curves_one_vs_rest(model_multiclass, data_generator, steps, output = 'results'):\n",
    "    '''\n",
    "    Plots the probability distributions and the ROC curves for each class of a one vs rest multiclass classification model.\n",
    "\n",
    "    Parameters:\n",
    "    model_multiclass: A multiclass classification model.\n",
    "    data_generator: A data generator that generates test data.\n",
    "    steps: Number of steps to run the generator for.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "\n",
    "    CreateDir(output)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bins = [i / 20 for i in range(20)] + [1]\n",
    "    classes = model_multiclass.classes_\n",
    "    roc_auc_ovr = {}\n",
    "    \n",
    "    # Generate X_test and y_test using the data generator\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(steps):\n",
    "        batch = next(data_generator)\n",
    "        X_test.extend(batch[0])\n",
    "        y_test.extend(batch[1])\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Get y_proba from the model\n",
    "    y_proba = model_multiclass.predict_proba(X_test)\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        # Gets the class\n",
    "        c = classes[i]\n",
    "\n",
    "        # Prepares an auxiliar dataframe to help with the plots\n",
    "        df_aux = pd.DataFrame(X_test.copy(), columns=['feature_{}'.format(i) for i in range(X_test.shape[1])])\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in y_test]\n",
    "        df_aux['prob'] = y_proba[:, i]\n",
    "        df_aux = df_aux.reset_index(drop=True)\n",
    "\n",
    "        # Plots the probability distribution for the class and the rest\n",
    "        ax = plt.subplot(2, 3, i + 1)\n",
    "        sns.histplot(x=\"prob\", data=df_aux, hue='class', color='b', ax=ax, bins=bins)\n",
    "        ax.set_title(c)\n",
    "        ax.legend([f\"Class: {c}\", \"Rest\"])\n",
    "        ax.set_xlabel(f\"P(x = {c})\")\n",
    "\n",
    "        # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "        ax_bottom = plt.subplot(2, 3, i + 4)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "        plot_roc_curve(tpr, fpr, scatter=False, ax=ax_bottom)\n",
    "        ax_bottom.set_title(\"ROC Curve OvR\")\n",
    "\n",
    "        # Calculates the ROC AUC OvR\n",
    "        roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d8b11-30ef-4eb2-af61-7333170c471e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

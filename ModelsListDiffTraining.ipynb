{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc98871-f506-414a-a914-a50044b9c7e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section A: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb9693-8729-4185-b0ee-6076c8129453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import import_ipynb\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def add_path_to_sys(path):\n",
    "    module_path = os.path.abspath(path)\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "usePath = os.path.join(r'c:', os.sep,'Users','scrwh','Documents','PythonScripts')\n",
    "add_path_to_sys(usePath)\n",
    "\n",
    "\n",
    "import ModelsListDiffFuntions\n",
    "from ModelsListDiffFuntions import *\n",
    "\n",
    "# List all the functions defined in the other notebook\n",
    "print(dir(ModelsListDiffFuntions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f30c3d-65bb-4ae7-8ad8-24a2318c3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc_memory_info = get_system_memory()\n",
    "print(\"PC RAM memory:\",pc_memory_info,\"\\n\")\n",
    "intel_gpu_memory_info = get_intel_gpu_memory()\n",
    "print(\"intel_gpu_memory_info:\",intel_gpu_memory_info,\"\\n\")\n",
    "nvidia_gpu_memory_info = get_nvidia_gpu_memory()\n",
    "print(\"nvidia_gpu_memory_info:\",nvidia_gpu_memory_info)\n",
    "\n",
    "tpu_memory_info = get_tpu_memory()\n",
    "print(\"TPU memory:\", tpu_memory_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ce4fc-5541-4338-aecc-a34c0eb8f3c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790c0e5-5837-4c92-b5a7-7bf7632c50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model(data_dir, batch_size=32, epochs=10, img_size=244, num_classes=9):\n",
    "    \n",
    "    # Build the model architecture\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(img_size, img_size, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d64aa-2499-431c-9441-7744452fcfe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_models(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData1').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU available, training on GPU...\")\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "    else:\n",
    "        print(\"GPU not available, training on CPU...\")\n",
    "        device_name = \"/CPU:0\"\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "    # Model creation\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "              ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "              ('VGG', VGG16, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    \n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        if model_fn == custom_model:\n",
    "            model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "        else:\n",
    "            base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "            x = Flatten()(base_model.output)\n",
    "            x = Dense(units=512, activation='relu')(x)\n",
    "            x = Dense(units=256, activation='relu')(x)\n",
    "            output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "            model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        # optimizer = Adam(learning_rate=0.001)\n",
    "        # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Move the model to the GPU if available\n",
    "        with tf.device(device_name):\n",
    "            # Define the learning rate schedule\n",
    "            def lr_schedule(epoch):\n",
    "                learning_rate = 0.0001\n",
    "                if epoch > 30:\n",
    "                    learning_rate *= 0.1\n",
    "                elif epoch > 20:\n",
    "                    learning_rate *= 0.01\n",
    "                print('Learning rate:', learning_rate)\n",
    "                print(get_nvidia_gpu_memory())\n",
    "                return learning_rate\n",
    "\n",
    "            # Define the callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model1.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            # print('Training Model:',name)\n",
    "            print(f'Training Model: {name}')\n",
    "            start_times = datetime.now()\n",
    "            print(f'{name} Started: {start_times}')\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=epochs,\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=len(val_generator),\n",
    "                callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "                )        \n",
    "            end_times = datetime.now()\n",
    "            print(f'{name} Ended: {end_times}')\n",
    "            print(f'Duration: {end_times - start_times}')\n",
    "            \n",
    "            # Run the garbage collector\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "            # save your model and its history to disk\n",
    "            model.save(f'{Folder}/wind_turbine_{name}_model1_{now}.h5')\n",
    "            with open(f'{Folder}/wind_turbine_{name}_history1_{now}.pkl', 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData1').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f78d4e-d95a-4076-8d24-ec01c576c70a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Note\n",
    "- data4 = Original sizes\n",
    "- data4b = 256\n",
    "- data4c = 512\n",
    "- data4d = 244\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3ed11-d250-4348-a811-4abb768fa05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join('..','..', 'data', 'data4d').replace(os.path.sep, '/')\n",
    "all_files = get_file_list(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9689d36-30c1-45f1-9064-54678417ab93",
   "metadata": {},
   "source": [
    "## Models 1 Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70b44861-457b-46c6-a7c2-00294618390c",
   "metadata": {},
   "source": [
    "# Generate the Folder with the current date and time\n",
    "foldername = os.path.join('Models','TrainingData').replace(os.path.sep, '/')\n",
    "now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "Folder = f'{foldername}_{now}'\n",
    "# os.makedirs(Folder, exist_ok=True)\n",
    "name = 'checking'\n",
    "\n",
    "os.path.join(Folder,name).replace(os.path.sep, '/')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77286464-05a8-48bb-9221-c6c84452a462",
   "metadata": {},
   "source": [
    "# Generate the filename with the current date and time\n",
    "name = os.path.join(Folder,'TrainingData')\n",
    "now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "filename = f'{name}_{now}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ebabf-13a1-4ceb-9a47-c6f7864101a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join('..', '..','data', 'data4d').replace(os.path.sep, '/')``````\n",
    "start_times = datetime.now()\n",
    "\n",
    "data1 = evaluate_models(data_folder, batch_size=32, epochs=50, img_size=244)\n",
    "\n",
    "end_times = datetime.now()\n",
    "print(f'\\nFinal Duration: {end_times - start_times}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8fc18-eff0-4ae5-9bff-5ec188f74acb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models 2\n",
    "- Use distributed training\n",
    "- multiple GPUs and want to take advantage of distributed training to potentially speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61eb388-9cf9-4dc3-b4dc-e6ce63473362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models1b(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData1b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Determine device to use\n",
    "    devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif len(devices) == 1:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "    # Model creation\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "              ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "              ('VGG16', VGG16, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        with strategy.scope():\n",
    "            if model_fn == custom_model:\n",
    "                model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "            else:\n",
    "                base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "                x = Flatten()(base_model.output)\n",
    "                x = Dense(units=512, activation='relu')(x)\n",
    "                x = Dense(units=256, activation='relu')(x)\n",
    "                output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "                model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Define the learning rate schedule\n",
    "        def lr_schedule(epoch):\n",
    "            learning_rate = 0.0001\n",
    "            if epoch > 30:\n",
    "                learning_rate *= 0.1\n",
    "            elif epoch > 20:\n",
    "                learning_rate *= 0.01\n",
    "            print('Learning rate:', learning_rate)\n",
    "            print(get_nvidia_gpu_memory())\n",
    "            return learning_rate\n",
    "\n",
    "        # Define the callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model1b_{now}.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        # Train the model\n",
    "        # print('Training Model:',name)\n",
    "        print(f'Training Model: {name}')\n",
    "        start_times = datetime.now()\n",
    "        print(f'{name} Started: {start_times}')\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(train_generator),\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=len(val_generator),\n",
    "            callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "            )        \n",
    "        end_times = datetime.now()\n",
    "        print(f'{name} Ended: {end_times}')\n",
    "        print(f'Duration: {end_times - start_times}')\n",
    "        \n",
    "        # Run the garbage collector\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        # save your model and its history to disk\n",
    "        model.save(f'{Folder}/wind_turbine_{name}_model1b_{now}.h5')\n",
    "        with open(f'{Folder}/wind_turbine_{name}_history1b_{now}.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData1b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f526c-9d31-4691-8a9c-ba20dfdb20c3",
   "metadata": {},
   "source": [
    "### Models 2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46b140-02eb-4e8a-945b-312890979a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = datetime.now()\n",
    "data1b = evaluate_models1b(data_folder, batch_size=32, epochs=50, img_size=244)\n",
    "\n",
    "end_times = datetime.now()\n",
    "print(f'\\nFinal Duration: {end_times - start_times}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04ac9a22-6daa-4151-8d71-7796251ce462",
   "metadata": {},
   "source": [
    "# Generate the filename with the current date and time\n",
    "name = os.path.join(Folder,'TrainingData1b').replace(os.path.sep, '/')\n",
    "now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "filename = f'{name}_{now}.csv'\n",
    "\n",
    "# Save the DataFrame to the CSV file\n",
    "data1b.to_csv(filename, index=True)\n",
    "\n",
    "print(f'Saved DataFrame to {filename}')\n",
    "\n",
    "data1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108961fe-385f-49f8-a3f2-2f982d64b6d6",
   "metadata": {},
   "source": [
    "## Models 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf1867-f761-4be8-9944-14e6ae7a2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_modelB(data_dir, batch_size=32, epochs=10, img_size=244, num_classes=9):\n",
    "    \n",
    "    # Build the model architecture\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(img_size, img_size, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fc954-221c-4be5-b402-2481281083c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modelsb(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData2').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU available, training on GPU...\")\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "    else:\n",
    "        print(\"GPU not available, training on CPU...\")\n",
    "        device_name = \"/CPU:0\"\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "    # Model creation\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "    #           ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "    #           ('VGG', VGG16, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    \n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        if model_fn == custom_model:\n",
    "            model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "        else:\n",
    "            base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "            x = Flatten()(base_model.output)\n",
    "            x = Dense(units=1024, activation='relu')(x)\n",
    "            x = Dense(units=512, activation='relu')(x)\n",
    "            output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "            model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        # optimizer = Adam(learning_rate=0.001)\n",
    "        # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Move the model to the GPU if available\n",
    "        with tf.device(device_name):\n",
    "            # Define the learning rate schedule\n",
    "            def lr_schedule(epoch):\n",
    "                learning_rate = 0.0001\n",
    "                if epoch > 30:\n",
    "                    learning_rate *= 0.1\n",
    "                elif epoch > 20:\n",
    "                    learning_rate *= 0.01\n",
    "                print('Learning rate:', learning_rate)\n",
    "                print(get_nvidia_gpu_memory())\n",
    "                return learning_rate\n",
    "\n",
    "            # Define the callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            # print('Training Model:',name)\n",
    "            print(f'Training Model: {name}')\n",
    "            start_times = datetime.now()\n",
    "            print(f'{name} Started: {start_times}')\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=epochs,\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=len(val_generator),\n",
    "                callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "                )        \n",
    "            end_times = datetime.now()\n",
    "            print(f'{name} Ended: {end_times}')\n",
    "            print(f'Duration: {end_times - start_times}')\n",
    "            \n",
    "            # Run the garbage collector\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "            # save your model and its history to disk\n",
    "            model.save(f'{Folder}/wind_turbine_{name}_model2_{now}.h5')\n",
    "            with open(f'{Folder}/wind_turbine_{name}_history2_{now}.pkl', 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData2').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299edee-a460-4ebb-bf67-85bd87786fa4",
   "metadata": {},
   "source": [
    "## Models 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6bb54-c2a3-4096-bd5a-f702c0de5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_times = datetime.now()\n",
    "\n",
    "data2 = evaluate_modelsb(data_folder, batch_size=32, epochs=50, img_size=244)\n",
    "\n",
    "end_times = datetime.now()\n",
    "print(f'\\nFinal Duration: {end_times - start_times}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "803ea46b-4185-4f83-b62b-0a73ca5e52ea",
   "metadata": {},
   "source": [
    "# Generate the filename with the current date and time\n",
    "name = os.path.join(Folder,'TrainingData2').replace(os.path.sep, '/')\n",
    "now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "filename = f'{name}_{now}.csv'\n",
    "\n",
    "# Save the DataFrame to the CSV file\n",
    "data2.to_csv(filename, index=True)\n",
    "\n",
    "print(f'Saved DataFrame to {filename}')\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c7963-3876-4296-8ec6-8c32ffc65c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # write the dataframe to a csv file\n",
    "# data2.to_csv('data2.csv', index=True)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b3d97-66ad-4119-aef2-623ee884094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89ea47-086b-4da6-b7b0-3948f76d3fc3",
   "metadata": {},
   "source": [
    "## Models 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22770dcb-5a05-4d24-90dd-e3ace7480b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modelsb2(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData2b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Determine device to use\n",
    "    devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif len(devices) == 1:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "    # Model creation\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "    #           ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "    #           ('VGG16', VGG16, 'imagenet'), ('Custom', custom_modelB, None)]\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        with strategy.scope():\n",
    "            if model_fn == custom_modelB:\n",
    "                model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "            else: \n",
    "                base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "                x = Flatten()(base_model.output)\n",
    "                x = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "                x = Dropout(0.5)(x)\n",
    "                x = Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "                x = Dropout(0.5)(x)\n",
    "                output = Dense(units=train_generator.num_classes, activation='softmax')(x)\n",
    "                model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # Define the learning rate schedule\n",
    "        def lr_schedule(epoch):\n",
    "            learning_rate = 0.0001\n",
    "            if epoch > 30:\n",
    "                learning_rate *= 0.1\n",
    "            elif epoch > 20:\n",
    "                learning_rate *= 0.01\n",
    "            print('Learning rate:', learning_rate)\n",
    "            print(get_nvidia_gpu_memory())\n",
    "            return learning_rate\n",
    "\n",
    "        # Define the callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model2b_{now}.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        # Train the model\n",
    "        # print('Training Model:',name)\n",
    "        print(f'Training Model: {name}')\n",
    "        start_times = datetime.now()\n",
    "        print(f'{name} Started: {start_times}')\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(train_generator),\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=len(val_generator),\n",
    "            callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "            )        \n",
    "        end_times = datetime.now()\n",
    "        print(f'{name} Ended: {end_times}')\n",
    "        print(f'Duration: {end_times - start_times}')\n",
    "        \n",
    "        # Run the garbage collector\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        # save your model and its history to disk\n",
    "        model.save(f'{Folder}/wind_turbine_{name}_model2b_{now}.h5')\n",
    "        with open(f'{Folder}/wind_turbine_{name}_history2b_{now}.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df['Average'] = df.select_dtypes(include='number').mean(axis=1)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData2b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f945c60-a8c5-413f-9852-e2c319e7dcaf",
   "metadata": {},
   "source": [
    "## Models 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31baddfe-7bb7-4deb-aa2a-e483233315c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_times = datetime.now()\n",
    "\n",
    "data2b = evaluate_modelsb2(data_folder, batch_size=32, epochs=50, img_size=244)\n",
    "\n",
    "end_times = datetime.now()\n",
    "print(f'\\nFinal Duration: {end_times - start_times}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f79ccd-9f1b-4dc0-8f13-7314efdb0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48cf7b-76a5-43c5-a9e2-0414801ec0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

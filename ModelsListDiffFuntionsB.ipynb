{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e29b781-868b-466a-9d87-41a782564069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import graphviz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import datetime\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import random\n",
    "import colorsys\n",
    "import webcolors\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.gridspec as gridspec\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import img_to_array,load_img\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers, regularizers, mixed_precision\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50, ResNet101, InceptionResNetV2, VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, UpSampling2D, concatenate, Activation,GlobalAveragePooling2D, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.layers import *\n",
    "# from tensorflow.contrib \n",
    "\n",
    "# from tensorflow.python.keras.utils.test_utils import get_test_data\n",
    "\n",
    "# from keras.utils.kpl_test_utils import get_test_data\n",
    "# from keras.utils.text_dataset import get_test_data\n",
    "# from keras.contrib.utils import test_utils\n",
    "# from keras.utils.test_utils import get_test_data\n",
    "import keras\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "# from tensorflow.keras_sequential_ascii import keras2ascii\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import visualkeras\n",
    "import pytest\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pycore.tikzeng import *\n",
    "from pycore.blocks  import *\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba30bf8d-241b-4da6-b29a-1a91575cc89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import pkg_resources\n",
    "\n",
    "# # Get a set of all packages installed in the environment\n",
    "# packages = {dist.project_name for dist in pkg_resources.working_set}\n",
    "\n",
    "# # Get a set of all imported packages\n",
    "# imported_packages = {pkg.key for pkg in pkg_resources.working_set if pkg.key in sys.modules}\n",
    "\n",
    "# # Filter out the packages that are not imported in the notebook\n",
    "# packages_in_notebook = imported_packages.intersection(packages)\n",
    "\n",
    "# # Print the list of imported packages\n",
    "# print(\"Imported packages in this notebook:\")\n",
    "# for pkg in sorted(packages_in_notebook):\n",
    "#     # print(pkg)\n",
    "#     x = f\"pip install {pkg}\"\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee41f2e-bf90-4e73-915a-ee37785c4b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pkg_resources\n",
    "\n",
    "# # get a list of all installed packages\n",
    "# installed_packages = pkg_resources.working_set\n",
    "\n",
    "# # get a list of imported packages\n",
    "# imported_packages = [i.key for i in installed_packages]\n",
    "\n",
    "# print(imported_packages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d203f5-5f35-429a-94e6-8d35b5a5353d",
   "metadata": {},
   "source": [
    "# Useful Rmarkdown link\n",
    "- https://eddjberry.netlify.app/post/writing-your-thesis-with-bookdown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a91e35-3c4f-480b-9073-d764362ba9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_count(folder):\n",
    "    return sum(len(files) for _, _, files in os.walk(folder))\n",
    "\n",
    "def sample_list(trend, lists, size):\n",
    "    data_list = [file for file in lists if trend in file]\n",
    "    return random.sample(data_list, min(size, len(data_list)))\n",
    "\n",
    "def get_file_list(data_folder):\n",
    "    return [os.path.join(dirpath, file) for dirpath, dirnames, files in os.walk(data_folder) for file in files]\n",
    "\n",
    "def generate_data(path, size, batch_size, class_mode, seed):\n",
    "    data = ImageDataGenerator(rescale=1/255)\n",
    "    return data.flow_from_directory(path,\n",
    "                                    target_size=(size, size),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode=class_mode,\n",
    "                                    seed=seed)\n",
    "\n",
    "\n",
    "def plot_image_grid(imglist):\n",
    "    rows = 3 if len(imglist) < 7 else 4\n",
    "    numm = math.ceil(len(imglist) / rows)\n",
    "    dim = (256, 256)\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    gs1 = gridspec.GridSpec(numm, rows)\n",
    "    gs1.update(wspace=0.025, hspace=0.08)\n",
    "\n",
    "    for idx, img_path in enumerate(imglist):\n",
    "        shp = Image.open(img_path).size\n",
    "        title = f'Image{idx} {shp}'\n",
    "        img = Image.open(img_path).resize(dim)\n",
    "        ax1 = plt.subplot(gs1[idx])\n",
    "        plt.title(title)\n",
    "        plt.grid(False)\n",
    "        plt.axis('on')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_aspect('equal')\n",
    "        plt.imshow(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_sample(elements, n, seed=1234, replace=0):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    if replace:\n",
    "        return random.choices(elements, k=n)\n",
    "    else:\n",
    "        return random.sample(elements, k=n)\n",
    "    \n",
    "    import os\n",
    "import random\n",
    "\n",
    "def get_random_samples(directory, num_samples, seed=1234, replace=0):\n",
    "    \"\"\"\n",
    "    Get m random samples from n sub-directories of a directory, ensuring at least one sample is chosen from each sub-directory\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    subdirectories = [subdir for subdir in os.listdir(directory) if os.path.isdir(os.path.join(directory, subdir))]\n",
    "    num_subdirectories = len(subdirectories)\n",
    "    samples = []\n",
    "    \n",
    "    # Ensure at least one sample is chosen from each sub-directory\n",
    "    for subdir in subdirectories:\n",
    "        files = os.listdir(os.path.join(directory, subdir))\n",
    "        if files:\n",
    "            file = os.path.join(directory, subdir, random.choice(files))\n",
    "            samples.append(file)\n",
    "    \n",
    "    # Choose remaining samples\n",
    "    remaining_samples = num_samples - len(samples)\n",
    "    if remaining_samples > 0:\n",
    "        for i in range(remaining_samples):\n",
    "            subdir = random.choice(subdirectories)\n",
    "            files = os.listdir(os.path.join(directory, subdir))\n",
    "            if files:\n",
    "                file = os.path.join(directory, subdir, random.choice(files))\n",
    "                samples.append(file)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "\n",
    "# def FolderTree(FolderPath):\n",
    "#     for root, dirs, files in os.walk(FolderPath):\n",
    "#         level = root.replace(FolderPath, '').count(os.sep)\n",
    "#         indent = ' ' * 4 * level\n",
    "#         print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "#         subindent = ' ' * 4 * (level + 1)\n",
    "#         num_files = len(files)\n",
    "#         print('{}{} file(s)'.format(subindent, num_files))\n",
    "\n",
    "def FolderTree(FolderPath):\n",
    "    for root, dirs, files in os.walk(FolderPath):\n",
    "        level = root.count(os.sep) - FolderPath.count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        num_files = len(files)\n",
    "        print(f\"{indent}{os.path.basename(root)}/ ({num_files} file{'s' if num_files != 1 else ''})\")\n",
    "        \n",
    "# import os\n",
    "\n",
    "def FileTree(FolderPath):\n",
    "    for root, dirs, files in os.walk(FolderPath):\n",
    "        level = root.count(os.sep) - FolderPath.count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        num_files = len(files)\n",
    "        print(f\"{indent}{os.path.basename(root)}/ ({num_files} file{'s' if num_files != 1 else ''})\")\n",
    "        for file in files:\n",
    "            file_indent = ' ' * 4 * (level+1)\n",
    "            print(f\"{file_indent}{file}\")\n",
    "\n",
    "        \n",
    "        \n",
    "def calculate_steps_per_epoch_and_epochs(num_samples, desired_epochs):\n",
    "    batch_size = 128  # example batch size\n",
    "    steps_per_epoch = num_samples // batch_size + 1\n",
    "    epochs = desired_epochs\n",
    "    while steps_per_epoch * epochs < num_samples:\n",
    "        epochs += 1\n",
    "    return steps_per_epoch, epochs\n",
    "\n",
    "def calculate_batch_size_steps_per_epoch_and_epochs(sample, desired_epochs):\n",
    "    num_samples = len(sample)\n",
    "    batch_size = train_generator.batch_size\n",
    "    steps_per_epoch = num_samples // batch_size + 1\n",
    "    epochs = desired_epochs\n",
    "    while steps_per_epoch * epochs < num_samples:\n",
    "        epochs += 1\n",
    "    return batch_size, steps_per_epoch, epochs\n",
    "\n",
    "def calculate_batch_size_steps_per_epoch_and_epochsb(train_generator, desired_epochs):\n",
    "    num_samples = train_generator.samples\n",
    "    batch_size = train_generator.batch_size\n",
    "    steps_per_epoch = num_samples // batch_size + 1\n",
    "    epochs = desired_epochs\n",
    "    while steps_per_epoch * epochs < num_samples:\n",
    "        epochs += 1\n",
    "    return batch_size, steps_per_epoch, epochs\n",
    "\n",
    "def generate_training_params(num_train, num_val, num_test):\n",
    "    # Batch size\n",
    "    batch_size = 32 if num_train <= 10000 else 38\n",
    "    # batch_size = 32 if num_train <= 10000 else 64\n",
    "    \n",
    "    # Epochs\n",
    "    epochs = 25 if num_train <= 10000 else 28\n",
    "    # epochs = 25 if num_train <= 10000 else 50\n",
    "    \n",
    "    # Steps per epoch\n",
    "    steps_per_epoch = num_train // batch_size\n",
    "    \n",
    "    # Validation steps\n",
    "    validation_steps = num_val // batch_size\n",
    "    \n",
    "    return batch_size, epochs, steps_per_epoch, validation_steps\n",
    "\n",
    "\n",
    "\n",
    "def load_data_generator(datagen, data_dir, subdir, img_size = 244, batch_size = 32):\n",
    "    if subdir != 'test':\n",
    "        shuffles=True\n",
    "    else:\n",
    "        shuffles=False\n",
    "    return datagen.flow_from_directory(\n",
    "        os.path.join(data_dir, subdir),\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle = shuffles\n",
    "        # class_mode='categorical'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def generate_colors(n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    # Define major colors in order of perceived visibility\n",
    "    major_colors = [\"blue\", \"orange\",\"black\",\"red\", \"green\", \"yellow\", \"purple\", \"pink\"]\n",
    "    # Convert major colors to HTML color codes and RGB values\n",
    "    major_colors_hex_rgb = [(webcolors.name_to_hex(color), webcolors.name_to_rgb(color)) for color in major_colors]\n",
    "    # Generate colors\n",
    "    hues = [random.uniform(0, 1) for _ in range(n - len(major_colors))]\n",
    "    saturations = [random.uniform(0.5, 1) for _ in range(n - len(major_colors))]\n",
    "    lightnesses = [random.uniform(0.4, 0.8) for _ in range(n - len(major_colors))]\n",
    "    hue_offsets = [random.choice([-0.2, 0, 0.2]) for _ in range(n - len(major_colors))]\n",
    "    colors_hls = [(hues[i]+hue_offsets[i], saturations[i], lightnesses[i]) for i in range(n - len(major_colors))]\n",
    "    colors_rgb = [tuple(round(c*255) for c in colorsys.hls_to_rgb(*hls)) for hls in colors_hls]\n",
    "    # Sort colors by proximity to the most distinguishable and visible colors\n",
    "    colors_hex_rgb = sorted(major_colors_hex_rgb + list(zip(map(webcolors.rgb_to_hex, colors_rgb), colors_rgb)), key=lambda c: min([abs(colorsys.rgb_to_hls(*c[1])[0] - colorsys.rgb_to_hls(*webcolors.name_to_rgb(color))[0]) for color in major_colors]))\n",
    "    # Convert RGB values to HTML color codes and return the first n colors\n",
    "    colors_hex = [c[0] for c in colors_hex_rgb]\n",
    "    return colors_hex[:n]\n",
    "\n",
    "\n",
    "def get_image_paths(image_path):\n",
    "    if isinstance(image_path, list):\n",
    "        image_pathss = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_pathss = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_pathss = [image_path]\n",
    "    return image_pathss\n",
    "\n",
    "\n",
    "def detect_faults(model, image_path, class_names, img_size=256,threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: List of tuples representing the fault predictions\n",
    "        (fault_name, x1, y1, x2, y2, confidence)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                x1, y1, x2, y2 = (10 * i, 10 * i, 200 - 10 * i, 200 - 10 * i)\n",
    "                colord = colors[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1, f'{class_names[i]} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "\n",
    "        ax.set_title(path.split('\\\\')[-2])\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        predictions = [(class_names[i], x1, y1, x2, y2, prediction)\n",
    "                       for i, prediction in enumerate(predictions[0])\n",
    "                       if prediction > 0.5]\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def getModelName(dateTime,s = None):\n",
    "    if s is None:\n",
    "        Custom_modelh5 = f'Models/TrainingData_{dateTime}/wind_turbine_Custom_model_{dateTime}.h5'\n",
    "        Custom_historypkl = f'Models/TrainingData_{dateTime}/wind_turbine_Custom_history_{dateTime}.pkl'\n",
    "        Inception_ResNet_V2_modelh5 = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-ResNet-V2_model_{dateTime}.h5'\n",
    "        Inception_ResNet_V2_historypkl = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-ResNet-V2_history_{dateTime}.pkl'\n",
    "        Inception_V2_modelh5 = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-V2_model_{dateTime}.h5'\n",
    "        Inception_V2_historypkl = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-V2_history_{dateTime}.pkl'\n",
    "        TrainingDatacsv = f'Models/TrainingData_{dateTime}/TrainingData_2023_{dateTime}.csv'\n",
    "    else:\n",
    "        Custom_modelh5 = f'Models/TrainingData_{dateTime}/wind_turbine_Custom{s}_model_{dateTime}.h5'\n",
    "        Custom_historypkl = f'Models/TrainingData_{dateTime}/wind_turbine_Custom{s}_history_{dateTime}.pkl'\n",
    "        Inception_ResNet_V2_modelh5 = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-ResNet-V2{s}_model_{dateTime}.h5'\n",
    "        Inception_ResNet_V2_historypkl = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-ResNet-V2{s}_history_{dateTime}.pkl'\n",
    "        Inception_V2_modelh5 = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-V2{s}_model_{dateTime}.h5'\n",
    "        Inception_V2_historypkl = f'Models/TrainingData_{dateTime}/wind_turbine_Inception-V2{s}_history_{dateTime}.pkl'\n",
    "        TrainingDatacsv = f'Models/TrainingData_{dateTime}/TrainingData{s}_2023_{dateTime}.csv'\n",
    "    return Custom_modelh5,Custom_historypkl,Inception_ResNet_V2_modelh5,Inception_ResNet_V2_historypkl,Inception_V2_modelh5,Inception_V2_historypkl,TrainingDatacsv\n",
    "\n",
    "#To get better visual of the confusion matrix:\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "             normalize=False,\n",
    "             title='Confusion matrix',\n",
    "             cmap=plt.cm.Blues):\n",
    "    #Add Normalization Option\n",
    "    '''prints pretty confusion metric with normalization option '''\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "#     print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "#To get better visual of the confusion matrix:\n",
    "def plot_confusion_matrixN(cm, classes,\n",
    "             normalize=False,\n",
    "             title='Confusion matrix',\n",
    "             cmap=plt.cm.Blues):\n",
    "    #Add Normalization Option\n",
    "    '''prints pretty confusion metric with normalization option '''\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        name = \"Normalized confusion matrix.png\"\n",
    "        print(\"Normalized_confusion_matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        name = \"Confusion_matrix_without_normalization.png\"\n",
    "    \n",
    "#     print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(name)\n",
    "    \n",
    "    \n",
    "def predictBar(model, image_path, classes, img_size=244, num_images=None):\n",
    "    \"\"\"\n",
    "    Predicts the class probabilities for a set of images using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model (tensorflow.keras.Model): The model to use for prediction.\n",
    "        image_path (str): The path to the directory/list containing the images to predict.\n",
    "        classes (list of str): A list of the class names in the order predicted by the model.\n",
    "        img_size (int, optional): The target size to resize the images. Defaults to 244.\n",
    "        num_images (int, optional): The number of images to plot the predictions for.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    imglist = get_image_paths(image_path)\n",
    "    num_images = num_images or len(imglist)\n",
    "    images = [img_to_array(load_img(img_path, target_size=(img_size, img_size))) / 255.0 for img_path in imglist[:num_images]]\n",
    "\n",
    "    # Predict the classes for the images\n",
    "    predictions = model.predict(np.array(images))\n",
    "\n",
    "    # Plot the images and predictions\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(16, 4 * num_images))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=-0.2)\n",
    "\n",
    "    for n in range(num_images):\n",
    "        # Plot the image\n",
    "        axes[n, 0].imshow(load_img(imglist[n], target_size=(299, 299)))\n",
    "        axes[n, 0].axis('off')\n",
    "        axes[n, 0].set_title(imglist[n].split(os.path.sep)[-2])\n",
    "\n",
    "        # Plot the predictions\n",
    "        bar_plot = axes[n, 1].bar(range(len(classes)), predictions[n])\n",
    "        axes[n, 1].set_xticks(range(len(classes)))\n",
    "        axes[n, 1].set_xticklabels(classes, rotation=15, ha=\"right\")\n",
    "        axes[n, 1].set_title(f\"Model prediction: {classes[np.argmax(predictions[n])]}\")\n",
    "\n",
    "        # Add text labels for predicted probabilities\n",
    "        for i, v in enumerate(predictions[n]):\n",
    "            if v > 0.004:\n",
    "                axes[n, 1].text(i, v, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "    plt.savefig('predictedBars.png', bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def predictBarColor(model, image_path, classes, img_size=244, num_images=None):\n",
    "    \"\"\"\n",
    "    Predicts the class probabilities for a set of images using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model (tensorflow.keras.Model): The model to use for prediction.\n",
    "        image_path (str): The path to the directory/list containing the images to predict.\n",
    "        classes (list of str): A list of the class names in the order predicted by the model.\n",
    "        img_size (int, optional): The target size to resize the images. Defaults to 244.\n",
    "        num_images (int, optional): The number of images to plot the predictions for.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    imglist = get_image_paths(image_path)\n",
    "    num_images = num_images or len(imglist)\n",
    "    images = [img_to_array(load_img(img_path, target_size=(img_size, img_size))) / 255.0 for img_path in imglist[:num_images]]\n",
    "\n",
    "    # Predict the classes for the images\n",
    "    predictions = model.predict(np.array(images))\n",
    "\n",
    "    # Define colors for each class\n",
    "    num_classes = len(classes)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, num_classes))\n",
    "\n",
    "    # Plot the images and predictions\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(16, 4 * num_images))\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=-0.2)\n",
    "    # fig.subplots_adjust(hspace=0.6, wspace=-0.2)\n",
    "\n",
    "    for n in range(num_images):\n",
    "        # Plot the image\n",
    "        axes[n, 0].imshow(load_img(imglist[n], target_size=(299, 299)))\n",
    "        axes[n, 0].axis('off')\n",
    "        axes[n, 0].set_title(imglist[n].split(os.path.sep)[-2])\n",
    "      \n",
    "        # Plot the predictions        \n",
    "        bar_plot = axes[n, 1].bar(range(len(classes)), predictions[n], color=colors)\n",
    "        axes[n, 1].set_xticks(range(len(classes)))\n",
    "        axes[n, 1].set_xticklabels(range(len(classes)))\n",
    "        # axes[n, 1].set_xticklabels(classes, rotation=20, ha=\"right\")\n",
    "        axes[n, 1].set_title(f\"Model prediction: {classes[np.argmax(predictions[n])]}\")\n",
    "\n",
    "        # Add text labels for predicted probabilities\n",
    "        for i, v in enumerate(predictions[n]):\n",
    "            if v > 0.004:\n",
    "                axes[n, 1].text(i, v, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "        # Create legend with class names and corresponding colors\n",
    "        legend_handles = [mpatches.Patch(color=colors[i], label=classes[i]) for i in range(len(classes))]\n",
    "        axes[n, 1].legend(handles=legend_handles, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "    plt.savefig('predictedBarsColor.png', bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b105acc-d2de-4255-a34b-4ba9a7dc6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHistory(history):\n",
    "    return history.history if 'history' in history else history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax1.plot(history['accuracy'])\n",
    "    ax1.plot(history['val_accuracy'])\n",
    "    ax1.set_title('Model accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    ax2.plot(history['loss'])\n",
    "    ax2.plot(history['val_loss'])\n",
    "    ax2.set_title('Model loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.savefig('training_history.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_training_historyB(history):\n",
    "    plt.plot(history[\"accuracy\"])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "\n",
    "    plt.savefig('training_historyB.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def EvaluateTest(data_dir, model, history, batch_size=32, img_size=256):\n",
    "\n",
    "    # Data augmentation for test set\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)    \n",
    "    # load_data_generator(datagen, data_dir, subdir, img_size = 244, batch_size = 32)\n",
    "    test_generator = load_data_generator(test_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "        \n",
    "    \n",
    "    # Create an empty dictionary to store time per image\n",
    "    time_per_image = {}\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    # print('\\n Evaluate the model on the test set')\n",
    "    start_time = time.time()\n",
    "    scores = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
    "    end_time = time.time()\n",
    "    # print(\"Test loss:\", scores[0])\n",
    "    # print(\"Test accuracy:\", scores[1])\n",
    "\n",
    "    # Calculate time per image in seconds\n",
    "    time_taken = end_time - start_time\n",
    "    time_per_image['test'] = time_taken / len(test_generator.filenames)\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_per_image, orient='index', columns=['Time per image (s)'])\n",
    "    print('EvaluateTime_report')\n",
    "    print(df)    \n",
    "    df.to_csv(\"EvaluateTime_report.csv\", index=True)\n",
    "    \n",
    "    \n",
    "    history = checkHistory(history)\n",
    "    \n",
    "    # get the index of the highest accuracy value\n",
    "    print('Training indexes')\n",
    "    highest_acc_index = np.argmax(history['val_accuracy'])\n",
    "    \n",
    "    # create a dictionary with the values\n",
    "    data = {'first': [history['loss'][0], history['accuracy'][0], \n",
    "                     history['val_loss'][0], history['val_accuracy'][0]],\n",
    "            'lowest': [min(history['loss']), min(history['accuracy']), \n",
    "                        min(history['val_loss']), min(history['val_accuracy'])],\n",
    "            'highest': [max(history['loss']), max(history['accuracy']), \n",
    "                        max(history['val_loss']), max(history['val_accuracy'])],\n",
    "            'last': [history['loss'][-1], history['accuracy'][-1], \n",
    "                     history['val_loss'][-1], history['val_accuracy'][-1]]\n",
    "           }\n",
    "    \n",
    "        # create a dataframe with the values as the index\n",
    "    df = pd.DataFrame(data, index=['training loss', 'training accuracy', 'validation loss', 'validation accuracy'])\n",
    "    # convert values to 4 decimal places\n",
    "    df = df.round(4)\n",
    "    # df = df.apply(lambda x: round(x, 4) if isinstance(x[1], float) else x)\n",
    "    # save the dataframe as CSV\n",
    "    # df.to_csv(os.path.join(output_dir, 'history.csv'), index=True)\n",
    "    df.to_csv('history.csv', index=True)\n",
    "    # print the dataframe\n",
    "    print(df,'\\n')\n",
    "\n",
    "    plot_training_history(history)\n",
    "\n",
    "    plot_training_historyB(history)\n",
    "    \n",
    "    classNmaes = list(test_generator.class_indices.keys())\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    print('\\n Evaluate the model on the test set')\n",
    "    # scores = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
    "    print(\"Test loss:\", scores[0])\n",
    "    print(\"Test accuracy:\", scores[1])\n",
    "    \n",
    "    # Get the true labels and the predicted labels from the test set\n",
    "    true_labels = test_generator.classes\n",
    "    predicted_labels = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "    # Compute the binary true labels and predicted labels for each class\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "    binary_true_labels = []\n",
    "    binary_predicted_labels = []\n",
    "    for i in range(len(class_names)):\n",
    "        class_idx = test_generator.class_indices[class_names[i]]\n",
    "        binary_true_labels.append((true_labels == class_idx).astype(int))\n",
    "        binary_predicted_labels.append(predicted_labels[:, i])\n",
    "\n",
    "    # Compute the ROC curve and AUC for each class\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    auc_roc_scores = []\n",
    "    for i in range(len(class_names)):\n",
    "        fpr, tpr, thresholds = roc_curve(binary_true_labels[i], binary_predicted_labels[i])\n",
    "        roc_auc = roc_auc_score(binary_true_labels[i], binary_predicted_labels[i])\n",
    "        auc_roc_scores.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=2, label=class_names[i] + ' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver operating characteristic ROC', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\")    \n",
    "    plt.savefig('auc_roc_scores.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"clases: \\n\", class_names)\n",
    "    # Compute the classification report    \n",
    "    predicted_labels = np.argmax(predicted_labels, axis=1)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)    \n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    df_cm = df_cm.round(2)\n",
    "    df_cm.to_csv('confusion_matrix.csv', index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cr = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
    "    cr_dict = classification_report(true_labels, predicted_labels, target_names=class_names, output_dict=True)\n",
    "    df_cr = pd.DataFrame(cr_dict).transpose()\n",
    "    df_cr = df_cr.round(2)\n",
    "    df_cr.to_csv(\"classification_report.csv\", index=True)\n",
    "    \n",
    "    print(\" \\nConfusion matrix:\\n\", cm)\n",
    "    print(df_cm)\n",
    "    \n",
    "    \n",
    "    # plot_confusion_matrix(cm, class_names)\n",
    "    plot_confusion_matrixN(cm, class_names,normalize=True)\n",
    "    \n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('Ground Truth label')    \n",
    "    name = \"Confusion_matrix_normalized.png\"\n",
    "    plt.savefig(name)\n",
    "    # plt.show()\n",
    "    \n",
    "    print(\" \\nClassification report:\\n\", cr)\n",
    "    print(df_cr)\n",
    "    \n",
    "\n",
    "    # Create a table of AUC-ROC scores for each class\n",
    "    auc_roc_table = pd.DataFrame({'Class': class_names, 'AUC-ROC': auc_roc_scores})\n",
    "    auc_roc_table = auc_roc_table.round(2)    \n",
    "    auc_roc_table.to_csv(\"auc_roc_table.csv\", index=False)\n",
    "    print(\"\\nauc_roc_table:\\n\", auc_roc_table)\n",
    "    \n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    true_labels_binary = lb.fit_transform(true_labels)\n",
    "    predicted_labels_binary = lb.transform(predicted_labels)\n",
    "\n",
    "    avg_precision = average_precision_score(true_labels_binary, predicted_labels_binary, average='macro')\n",
    "    print(\"Average Precision Score: \", avg_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f30c3d-65bb-4ae7-8ad8-24a2318c3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import platform\n",
    "import GPUtil\n",
    "import os\n",
    "import pynvml\n",
    "import subprocess\n",
    "\n",
    "def get_system_memory():\n",
    "    svmem = psutil.virtual_memory()\n",
    "    return {'memory_total': f\"{svmem.total/(1024**3):.2f} GB\", 'memory_used': f\"{svmem.used/(1024**3):.2f} GB\"}\n",
    "\n",
    "def get_intel_gpu_memory():\n",
    "    if platform.system() != 'Windows':\n",
    "        return None\n",
    "    meminfo = os.popen('wmic path win32_VideoController get AdapterRAM').read()\n",
    "    return {'memory_total': f\"{int(meminfo.split()[-1])/(1024**3):.2f} GB\"}\n",
    "\n",
    "def get_nvidia_gpu_memory():\n",
    "    cmd = 'nvidia-smi --query-gpu=memory.total,memory.used --format=csv,nounits,noheader'\n",
    "    output = subprocess.check_output(cmd, shell=True).decode('utf-8').strip()\n",
    "    gpu_memory = [line.split(',') for line in output.split('\\n')]\n",
    "    return [{'id': i, 'memory_total': f\"{int(mem[0])/1e3:.2f} GB\", 'memory_used': f\"{int(mem[1])/1e3:.2f} GB\"} for i, mem in enumerate(gpu_memory)]\n",
    "\n",
    "def get_tpu_memory():\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "        tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
    "        tpu_strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)\n",
    "        return {'memory_total': f\"{tpu_strategy.num_replicas_in_sync * 8:.2f} GB\"} # each TPU has 8GB memory\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2790c0e5-5837-4c92-b5a7-7bf7632c50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model(data_dir, batch_size=32, epochs=10, img_size=244, num_classes=9):\n",
    "    \n",
    "    # Build the model architecture\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(img_size, img_size, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5d64aa-2499-431c-9441-7744452fcfe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_models(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU available, training on GPU...\")\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "    else:\n",
    "        print(\"GPU not available, training on CPU...\")\n",
    "        device_name = \"/CPU:0\"\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    \n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        if model_fn == custom_model:\n",
    "            model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "        else:\n",
    "            base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "            x = Flatten()(base_model.output)\n",
    "            x = Dense(units=512, activation='relu')(x)\n",
    "            x = Dense(units=256, activation='relu')(x)\n",
    "            output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "            model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        # optimizer = Adam(learning_rate=0.001)\n",
    "        # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Move the model to the GPU if available\n",
    "        with tf.device(device_name):\n",
    "            # Define the learning rate schedule\n",
    "            def lr_schedule(epoch):\n",
    "                learning_rate = 0.0001\n",
    "                if epoch > 30:\n",
    "                    learning_rate *= 0.1\n",
    "                elif epoch > 20:\n",
    "                    learning_rate *= 0.01\n",
    "                print('Learning rate:', learning_rate)\n",
    "                print(get_nvidia_gpu_memory())\n",
    "                return learning_rate\n",
    "\n",
    "            # Define the callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            # print('Training Model:',name)\n",
    "            print(f'Training Model: {name}')\n",
    "            start_times = datetime.now()\n",
    "            print(f'{name} Started: {start_times}')\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=epochs,\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=len(val_generator),\n",
    "                callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "                )        \n",
    "            end_times = datetime.now()\n",
    "            print(f'{name} Ended: {end_times}')\n",
    "            print(f'Duration: {end_times - start_times}')\n",
    "            \n",
    "            # Run the garbage collector\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "            # save your model and its history to disk\n",
    "            model.save(f'{Folder}/wind_turbine_{name}_model_{now}.h5')\n",
    "            with open(f'{Folder}/wind_turbine_{name}_history_{now}.pkl', 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f78d4e-d95a-4076-8d24-ec01c576c70a",
   "metadata": {},
   "source": [
    "# Note\n",
    "- data4 = Original sizes\n",
    "- data4b = 256\n",
    "- data4c = 512\n",
    "- data4d = 244\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61eb388-9cf9-4dc3-b4dc-e6ce63473362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def evaluate_models1b(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData1b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "    # Determine device to use\n",
    "    devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif len(devices) == 1:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "    # Model creation\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "    #           ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "    #           ('VGG16', VGG16, 'imagenet'), ('Custom', custom_model, None)]\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_model, None)]\n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        with strategy.scope():\n",
    "            if model_fn == custom_model:\n",
    "                model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "            else:\n",
    "                base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "                x = Flatten()(base_model.output)\n",
    "                x = Dense(units=512, activation='relu')(x)\n",
    "                x = Dense(units=256, activation='relu')(x)\n",
    "                output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "                model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Define the learning rate schedule\n",
    "        def lr_schedule(epoch):\n",
    "            learning_rate = 0.0001\n",
    "            if epoch > 30:\n",
    "                learning_rate *= 0.1\n",
    "            elif epoch > 20:\n",
    "                learning_rate *= 0.01\n",
    "            print('Learning rate:', learning_rate)\n",
    "            print(get_nvidia_gpu_memory())\n",
    "            return learning_rate\n",
    "\n",
    "        # Define the callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_model1b_{now}.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        # Train the model\n",
    "        # print('Training Model:',name)\n",
    "        print(f'Training Model: {name}')\n",
    "        start_times = datetime.now()\n",
    "        print(f'{name} Started: {start_times}')\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(train_generator),\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=len(val_generator),\n",
    "            callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "            )        \n",
    "        end_times = datetime.now()\n",
    "        print(f'{name} Ended: {end_times}')\n",
    "        print(f'Duration: {end_times - start_times}')\n",
    "        \n",
    "        # Run the garbage collector\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        # save your model and its history to disk\n",
    "        model.save(f'{Folder}/wind_turbine_{name}_model1b_{now}.h5')\n",
    "        with open(f'{Folder}/wind_turbine_{name}_history1b_{now}.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData1b').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32974840-ec7a-4487-9345-cc008d1bff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_modelB(data_dir, batch_size=32, epochs=10, img_size=244, num_classes=9):\n",
    "    \n",
    "    # Build the model architecture\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(img_size, img_size, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12517809-bd4c-45ad-b20c-ca9f9e09f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modelsb(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    foldername = os.path.join('Models','TrainingData2').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "    \n",
    "    # Check if a GPU is available\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU available, training on GPU...\")\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "    else:\n",
    "        print(\"GPU not available, training on CPU...\")\n",
    "        device_name = \"/CPU:0\"\n",
    "        \n",
    "        \n",
    "\n",
    "    # Determine device to use\n",
    "    devices = tf.config.list_physical_devices('GPU')\n",
    "    print(\"Number of GPUs available: \", len(devices))\n",
    "    if len(devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif len(devices) == 1:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        \n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "    # Model creation\n",
    "    # models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "    #           ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "    #           ('VGG', VGG16, 'imagenet'), ('Custom', custom_modelB, None)]\n",
    "    \n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_modelB, None)]\n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        # with strategy.scope():\n",
    "            \n",
    "        if model_fn == custom_modelB:\n",
    "            model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "        else:\n",
    "            base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "            x = Flatten()(base_model.output)\n",
    "            x = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "            x = Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "            output = Dense(units=train_generator.num_classes, activation='softmax')(x)\n",
    "            model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        # optimizer = Adam(learning_rate=0.001)\n",
    "        # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Move the model to the GPU if available\n",
    "        with tf.device(device_name):\n",
    "            # Define the learning rate schedule\n",
    "            def lr_schedule(epoch):\n",
    "                learning_rate = 0.0001\n",
    "                if epoch > 30:\n",
    "                    learning_rate *= 0.1\n",
    "                elif epoch > 20:\n",
    "                    learning_rate *= 0.01\n",
    "                print('Learning rate:', learning_rate)\n",
    "                print(get_nvidia_gpu_memory())\n",
    "                return learning_rate\n",
    "\n",
    "            # Define the callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "            checkpoint = ModelCheckpoint(f'{Folder}/best_{name}2_model_{now}.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            # print('Training Model:',name)            \n",
    "            print(f'Training Model: {name}')\n",
    "            start_times = datetime.now()\n",
    "            print(f'{name} Started: {start_times}')\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=epochs,\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=len(val_generator),\n",
    "                callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "                )        \n",
    "            end_times = datetime.now()\n",
    "            print(f'{name} Ended: {end_times}')\n",
    "            print(f'Duration: {end_times - start_times}')\n",
    "            \n",
    "            # Run the garbage collector\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "            # save your model and its history to disk\n",
    "            model.save(f'{Folder}/wind_turbine_{name}2_model_{now}.h5')\n",
    "            with open(f'{Folder}/wind_turbine_{name}2_history_{now}.pkl', 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData2').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    return df\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5671170-4cfb-4440-9355-fbc592bd6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def evaluate_modelsb2b(data_dir, batch_size=32, epochs=10, img_size=244):\n",
    "    # Generate the Folder with the current date and time\n",
    "    # foldername = os.path.join('Models','TrainingData2c')\n",
    "    foldername = os.path.join('Models','TrainingData2c').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    Folder = f'{foldername}_{now}'\n",
    "    os.makedirs(Folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Determine device to use\n",
    "    devices = tf.config.list_physical_devices('GPU')\n",
    "    print(\"Number of GPUs available: \", len(devices))\n",
    "    if len(devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif len(devices) == 1:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # Data augmentation and generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,  width_shift_range=0.2,  height_shift_range=0.2,  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the training, validation and test set\n",
    "    train_generator = load_data_generator(train_datagen, data_dir, 'train', img_size = img_size, batch_size = batch_size)\n",
    "    val_generator = load_data_generator(val_datagen, data_dir, 'val', img_size = img_size, batch_size = batch_size)\n",
    "    test_generator = load_data_generator(val_datagen, data_dir, 'test', img_size = img_size, batch_size = batch_size)\n",
    "    \n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# #     # Model creation\n",
    "#     models = [('Inception-V2', InceptionV3, 'imagenet'), ('ResNet-50', ResNet50, 'imagenet'),\n",
    "#               ('ResNet-101', ResNet101, 'imagenet'), ('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'),\n",
    "#               ('VGG', VGG16, 'imagenet'), ('Customb', custom_modelB, None)]\n",
    "\n",
    "    models = [('Inception-V2', InceptionV3, 'imagenet'),('Inception-ResNet-V2', InceptionResNetV2, 'imagenet'), ('Custom', custom_modelB, None)]\n",
    "    \n",
    "\n",
    "    model_metrics = {'Model':[], 'Training Accuracy':[], 'Validation Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "    for name, model_fn, weights in models:\n",
    "        with strategy.scope():\n",
    "            if model_fn == custom_modelB:\n",
    "                model = model_fn(data_dir=data_dir, batch_size=batch_size, epochs=epochs, img_size=img_size, num_classes = num_classes)\n",
    "            else:\n",
    "                base_model = model_fn(input_shape=(img_size, img_size, 3), include_top=False, weights=weights)\n",
    "                x = Flatten()(base_model.output)\n",
    "                x = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "                x = Dropout(0.5)(x)\n",
    "                x = Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "                x = Dropout(0.5)(x)\n",
    "                output = Dense(units=train_generator.num_classes, activation='softmax')(x)\n",
    "                model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Define the learning rate schedule\n",
    "        def lr_schedule(epoch):\n",
    "            learning_rate = 0.0001\n",
    "            if epoch > 30:\n",
    "                learning_rate *= 0.1\n",
    "            elif epoch > 20:\n",
    "                learning_rate *= 0.01\n",
    "            print('Learning rate:', learning_rate)\n",
    "            print(get_nvidia_gpu_memory())\n",
    "            return learning_rate\n",
    "\n",
    "        # Define the callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        checkpoint = ModelCheckpoint(f'{Folder}/best_{name}_mode2c_{now}.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        # Train the model\n",
    "        # print('Training Model:',name)        \n",
    "        print(f'Training Model: {name}')\n",
    "        start_times = datetime.now()\n",
    "        print(f'{name} Started: {start_times}')\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(train_generator),\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=len(val_generator),\n",
    "            callbacks=[early_stopping, reduce_lr, lr_scheduler, checkpoint]\n",
    "            )        \n",
    "        end_times = datetime.now()\n",
    "        print(f'{name} Ended: {end_times}')\n",
    "        print(f'Duration: {end_times - start_times}')\n",
    "            \n",
    "        # Run the garbage collector\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "        # save your model and its history to disk\n",
    "        model.save(f'{Folder}/wind_turbine_{name}_mode2c_{now}.h5')\n",
    "        with open(f'{Folder}/wind_turbine_{name}_history2c_{now}.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "        # labels = list(train_generator.class_indices.keys())\n",
    "        # model.summary()        \n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        _, train_acc = model.evaluate(train_generator)\n",
    "        _, val_acc = model.evaluate(val_generator)\n",
    "        _, test_acc = model.evaluate(test_generator)\n",
    "        model_metrics['Model'].append(name)\n",
    "        model_metrics['Training Accuracy'].append(train_acc)\n",
    "        model_metrics['Validation Accuracy'].append(val_acc)\n",
    "        model_metrics['Test Accuracy'].append(test_acc)\n",
    "\n",
    "    # Saving the performance in a DataFrame\n",
    "    df = pd.DataFrame(model_metrics)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    \n",
    "    # Generate the filename with the current date and time\n",
    "    name = os.path.join(Folder,'TrainingData2c').replace(os.path.sep, '/')\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H%M')\n",
    "    filename = f'{name}_{now}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filename, index=True)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # print(f'Saved DataFrame to {filename}')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65057b6-0212-4978-ab49-ca7c0ecbf42e",
   "metadata": {},
   "source": [
    "# Testing individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92d88974-7b79-4258-9ceb-1c2231823f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def detect_faultsdfTrue(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size to resize to (default 256)\n",
    "    - threshold: Prediction threshold (default 0.2)\n",
    "\n",
    "    Returns:\n",
    "    - df: Pandas dataframe with columns 'Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'\n",
    "    \"\"\"\n",
    "    \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "            \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv('SamplePredicted.csv', index=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b97447-1703-4417-8164-47d5d50bd22e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def detect_faultsdf(model, image_path, class_names, img_size=256, threshold=0.2):  \n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size to resize to (default 256)\n",
    "    - threshold: Prediction threshold (default 0.2)\n",
    "    - n_cols: Number of columns in the final image grid (default 3)\n",
    "\n",
    "    Returns:\n",
    "    - df: Pandas dataframe with columns 'Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'\n",
    "    \"\"\"  \n",
    "    sepps = os.path.sep \n",
    "    pattern = r'[\\[\\](){}_\\s]'\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "    n_cols = 3 if len(image_paths) < 7 else 4\n",
    "\n",
    "    all_predictions = []\n",
    "    predicted_images = []\n",
    "    n_rows = int(np.ceil(len(image_paths) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "\n",
    "    colors = generate_colors(len(class_names))\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "                x1, y1, x2, y2 = (10 * j, 10 * j, 200 - 10 * j, 200 - 10 * j)\n",
    "                colord = colors[j]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1, f'{class_names[j]} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        ax.set_title(path.split(sepps)[-2])\n",
    "        ax.axis('off')\n",
    "\n",
    "        predicted_images.append(fig)\n",
    "        \n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence, prediction_time_in_sec))\n",
    "            \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "    df['Image'] = df['Image'].str.replace(pattern, '')    \n",
    "    df['Image'] = df['Image'].str.replace('[\\(\\)\\[\\]\\{\\}]', '')\n",
    "    df['Image'] = df['Image'].str[-9:]\n",
    "    df['Image'] = df['Image'].str.capitalize()\n",
    "\n",
    "    for i in range(len(image_paths), n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.005, hspace=0.15)  # modify the spacing between subplots\n",
    "    plt.savefig('final_image.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    # count number of rows with one predicted label\n",
    "    one_label = (df['Predicted Label'].str.count(',') == 0).sum()\n",
    "    # count number of rows with more than one predicted label\n",
    "    more_labels = (df['Predicted Label'].str.count(',') > 0).sum()\n",
    "    \n",
    "    total_Images = df.shape[0]\n",
    "    total_Predictions = df_split.shape[0]\n",
    "    # Assuming the variables are already defined\n",
    "    print(f\"\\n\\n\\nNumber of rows with one predicted label: {one_label}\")\n",
    "    print(f\"Number of rows with more than one predicted label: {more_labels}\")\n",
    "    print(f\"Total Images label: {total_Images}\")\n",
    "    print(f\"Total predicted label: {total_Predictions}\")\n",
    "\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Calculate percentage of accuracy and wrong predictions\n",
    "    accuracy = (count / total_Predictions) * 100\n",
    "    wrong_predictions = (countb / total_Predictions) * 100\n",
    "\n",
    "    print(f\"Percentage of accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Percentage of wrong predictions: {wrong_predictions:.2f}%\\n\\n\")\n",
    "    \n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv('SamplePredicted.csv', index=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0feac-7374-4ea4-829d-d4c63f4c4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics_multiclass(y_true, y_pred, class_names):\n",
    "    metrics = {}\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Create binary label arrays\n",
    "        y_true_class = (y_true == i).astype(int)\n",
    "        y_pred_class = (y_pred == i).astype(int)\n",
    "\n",
    "        # Calculate TP, FP, TN, FN\n",
    "        tp = np.sum(y_true_class * y_pred_class)\n",
    "        fp = np.sum((1 - y_true_class) * y_pred_class)\n",
    "        tn = np.sum((1 - y_true_class) * (1 - y_pred_class))\n",
    "        fn = np.sum(y_true_class * (1 - y_pred_class))\n",
    "\n",
    "        # Calculate TPR and FPR\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + tn)\n",
    "\n",
    "        # Save metrics for the class\n",
    "        metrics[class_name] = {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn, 'TPR': tpr, 'FPR': fpr}\n",
    "\n",
    "    # Convert metrics to DataFrame\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df = df.T\n",
    "    df.index.name = 'Class'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bcb4a-d5f8-4f3b-933c-07327e652478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faultsdfalone(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "        \n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence,prediction_time_in_sec))\n",
    "        \n",
    "        y_true.append(gt_label)\n",
    "        y_pred.append(pred_labels)\n",
    "            \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    one_label = (df['Predicted Label'].str.count(',') == 0).sum()\n",
    "    more_labels = (df['Predicted Label'].str.count(',') > 0).sum()\n",
    "    \n",
    "    total_Images = df.shape[0]\n",
    "    total_Predictions = df_split.shape[0]\n",
    "    # Assuming the variables are already defined\n",
    "    print(f\"\\n\\n\\nNumber of rows with one predicted label: {one_label}\")\n",
    "    print(f\"Number of rows with more than one predicted label: {more_labels}\")\n",
    "    print(f\"Total Images label: {total_Images}\")\n",
    "    print(f\"Total predicted label: {total_Predictions}\")\n",
    "\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Calculate percentage of accuracy and wrong predictions\n",
    "    accuracy = (count / total_Predictions) * 100\n",
    "    wrong_predictions = (countb / total_Predictions) * 100\n",
    "\n",
    "    print(f\"Percentage of accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Percentage of wrong predictions: {wrong_predictions:.2f}%\\n\\n\")\n",
    "\n",
    "    df.to_csv('SamplePredicted.csv', index=False)\n",
    "    \n",
    "    df2 = calculate_metrics_multiclass(y_true, y_pred, class_names)\n",
    "\n",
    "    return df, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e31251-d732-47f2-b6dc-5d4e817cd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def detect_faultsdfalone(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size to resize to (default 256)\n",
    "    - threshold: Prediction threshold (default 0.2)\n",
    "\n",
    "    Returns:\n",
    "    - df: Pandas dataframe with columns 'Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'\n",
    "    \"\"\"\n",
    "    \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "        \n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence,prediction_time_in_sec))\n",
    "            \n",
    "    # df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])    \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    # count number of rows with one predicted label\n",
    "    one_label = (df['Predicted Label'].str.count(',') == 0).sum()\n",
    "    # count number of rows with more than one predicted label\n",
    "    more_labels = (df['Predicted Label'].str.count(',') > 0).sum()\n",
    "    \n",
    "    total_Images = df.shape[0]\n",
    "    total_Predictions = df_split.shape[0]\n",
    "    # Assuming the variables are already defined\n",
    "    print(f\"\\n\\n\\nNumber of rows with one predicted label: {one_label}\")\n",
    "    print(f\"Number of rows with more than one predicted label: {more_labels}\")\n",
    "    print(f\"Total Images label: {total_Images}\")\n",
    "    print(f\"Total predicted label: {total_Predictions}\")\n",
    "\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Calculate percentage of accuracy and wrong predictions\n",
    "    accuracy = (count / total_Predictions) * 100\n",
    "    wrong_predictions = (countb / total_Predictions) * 100\n",
    "\n",
    "    print(f\"Percentage of accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Percentage of wrong predictions: {wrong_predictions:.2f}%\\n\\n\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv('SamplePredicted.csv', index=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a3969-f446-4782-b6f8-8319ebbc0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faultsdfaloneb(model, image_path, class_names, img_size=256, threshold=0.2):    \n",
    "    sepps = os.path.sep\n",
    "    \n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "        \n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence,prediction_time_in_sec))\n",
    "            \n",
    "    # df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])    \n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    # count number of rows with one predicted label\n",
    "    one_label = (df['Predicted Label'].str.count(',') == 0).sum()\n",
    "    # count number of rows with more than one predicted label\n",
    "    more_labels = (df['Predicted Label'].str.count(',') > 0).sum()\n",
    "    \n",
    "    total_Images = df.shape[0]\n",
    "    total_Predictions = df_split.shape[0]\n",
    "    # Assuming the variables are already defined\n",
    "    print(f\"\\n\\n\\nNumber of rows with one predicted label: {one_label}\")\n",
    "    print(f\"Number of rows with more than one predicted label: {more_labels}\")\n",
    "    print(f\"Total Images label: {total_Images}\")\n",
    "    print(f\"Total predicted label: {total_Predictions}\")\n",
    "\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Calculate percentage of accuracy and wrong predictions\n",
    "    accuracy = (count / total_Predictions) * 100\n",
    "    wrong_predictions = (countb / total_Predictions) * 100\n",
    "\n",
    "    print(f\"Percentage of accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Percentage of wrong predictions: {wrong_predictions:.2f}%\\n\\n\")\n",
    "\n",
    "    # Calculate TP, FP, TN, FN for each image\n",
    "    for image_name, df_image in df_split.groupby('Image'):\n",
    "        gt_labels = set(df_image['Ground_Truth_Label'].values)\n",
    "        pred_labels = set(df_image['Predicted_Label'].values)\n",
    "        TP = len(gt_labels.intersection(pred_labels))\n",
    "        FP = len(pred_labels - gt_labels)\n",
    "        TN = 0\n",
    "        FN = len(gt_labels - pred_labels)\n",
    "        df.loc[df['Image'] == image_name, 'TP'] = TP\n",
    "        df.loc[df['Image'] == image_name, 'FP'] = FP\n",
    "        df.loc[df['Image'] == image_name, 'TN'] = TN\n",
    "        df.loc[df['Image'] == image_name, 'FN'] = FN\n",
    "\n",
    "    # Calculate TPR and FPR\n",
    "    TP = df['TP'].sum()\n",
    "    FP = df['FP'].sum()\n",
    "    TN = df['TN'].sum()\n",
    "    FN = df['FN'].sum()\n",
    "\n",
    "\n",
    "\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    misclassification = 1 - accuracy\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    print(f\"TP: {TP:.2f}\")\n",
    "    print(f\"FP: {FP:.2f}\")\n",
    "    print(f\"TN: {TN:.2f}\")\n",
    "    print(f\"FN: {FN:.2f}\")\n",
    "    print(f\"TPR: {TPR:.2f}\")\n",
    "    print(f\"FPR: {FPR:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Misclassification: {misclassification:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"recall: {recall:.2f}\")\n",
    "    print(f\"specificity: {specificity:.2f}\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv('SamplePredictedb.csv', index=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cb921-8cbd-4e4b-8c4c-161ecca8aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faultsdfaloneb(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    sepps = os.path.sep\n",
    "\n",
    "    image_paths = get_image_paths(image_path)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(image)\n",
    "        end_time = time.time()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_time_in_sec = f\"{prediction_time:.2f}\"\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "\n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence, prediction_time_in_sec))\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence', 'Prediction Time (sec)'])\n",
    "\n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    # count number of rows with one predicted label\n",
    "    one_label = (df['Predicted Label'].str.count(',') == 0).sum()\n",
    "    # count number of rows with more than one predicted label\n",
    "    more_labels = (df['Predicted Label'].str.count(',') > 0).sum()\n",
    "    \n",
    "    total_Images = df.shape[0]\n",
    "    total_Predictions = df_split.shape[0]\n",
    "    # Assuming the variables are already defined\n",
    "    print(f\"\\n\\n\\nNumber of rows with one predicted label: {one_label}\")\n",
    "    print(f\"Number of rows with more than one predicted label: {more_labels}\")\n",
    "    print(f\"Total Images label: {total_Images}\")\n",
    "    print(f\"Total predicted label: {total_Predictions}\")\n",
    "\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    # Calculate percentage of accuracy and wrong predictions\n",
    "    accuracy = (count / total_Predictions) * 100\n",
    "    wrong_predictions = (countb / total_Predictions) * 100\n",
    "\n",
    "    print(f\"Percentage of accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Percentage of wrong predictions: {wrong_predictions:.2f}%\\n\\n\")\n",
    "\n",
    "    # Calculate TP, FP, TN, FN for each image\n",
    "    for image_name, df_image in df_split.groupby('Image'):\n",
    "        gt_labels = set(df_image['Ground_Truth_Label'].values)\n",
    "        pred_labels = set(df_image['Predicted_Label'].values)\n",
    "        TP = len(gt_labels.intersection(pred_labels))\n",
    "        FP = len(pred_labels - gt_labels)\n",
    "        TN = 0\n",
    "        FN = len(gt_labels - pred_labels)\n",
    "        df.loc[df['Image'] == image_name, 'TP'] = TP\n",
    "        df.loc[df['Image'] == image_name, 'FP'] = FP\n",
    "        df.loc[df['Image'] == image_name, 'TN'] = TN\n",
    "        df.loc[df['Image'] == image_name, 'FN'] = FN\n",
    "\n",
    "    # Calculate TPR and FPR\n",
    "    TP = df['TP'].sum()\n",
    "    FP = df['FP'].sum()\n",
    "    TN = df['TN'].sum()\n",
    "    FN = df['FN'].sum()\n",
    "\n",
    "\n",
    "\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    misclassification = 1 - accuracy\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    print(f\"TP: {TP:.2f}\")\n",
    "    print(f\"FP: {FP:.2f}\")\n",
    "    print(f\"TN: {TN:.2f}\")\n",
    "    print(f\"FN: {FN:.2f}\")\n",
    "    print(f\"TPR: {TPR:.2f}\")\n",
    "    print(f\"FPR: {FPR:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Misclassification: {misclassification:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"recall: {recall:.2f}\")\n",
    "    print(f\"specificity: {specificity:.2f}\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv('SamplePredictedb.csv', index=False)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(df_split['Ground_Truth_Label'], df_split['Predicted_Label'])\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(cm, cmap='Oranges')  # Change the color map\n",
    "    ax.set_title('Confusion Matrix', fontsize=16)  # Increase title font size\n",
    "    ax.set_xlabel('Predicted label', fontsize=14)  # Increase label font size\n",
    "    ax.set_ylabel('Ground Truth label', fontsize=14)  # Increase label font size\n",
    "    ax.set_xticks(np.arange(len(class_names)))\n",
    "    ax.set_yticks(np.arange(len(class_names)))\n",
    "    # ax.set_xticklabels(class_names, fontsize=12)  # Decrease tick label font size\n",
    "    ax.set_xticklabels(class_names, fontsize=12, rotation=15, ha='right')  # Decrease tick label font size and rotate xticklabels\n",
    "    ax.set_yticklabels(class_names, fontsize=12)  # Decrease tick label font size\n",
    "\n",
    "    # Add text to each cell with different font colors based on the background intensity\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            text_color = 'white' if cm[i, j] > cm.max() / 2 else 'black'  # Choose font color based on background intensity\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color=text_color, fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32582870-6f6c-4f34-af52-243226f1995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faults_images(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    sepps = os.path.sep\n",
    "\n",
    "    image_paths = get_image_paths(image_path)\n",
    "    n_cols = min(4, len(image_paths))\n",
    "\n",
    "    all_predictions = []\n",
    "    data = []\n",
    "\n",
    "    fig, axes = plt.subplots(len(image_paths), n_cols, figsize=(4 * n_cols, 4 * len(image_paths)))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []  \n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        ax = axes[i*n_cols:(i+1)*n_cols]\n",
    "        ax[0].imshow(np.squeeze(image))\n",
    "        ax[0].set_title(path.split(sepps)[-2])\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[j])\n",
    "                confidences.append(prediction)\n",
    "                \n",
    "                x1, y1, x2, y2 = (10 * j, 10 * j, 200 - 10 * j, 200 - 10 * j)\n",
    "                colord = colors[j]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax[0].add_patch(rect)\n",
    "                ax[0].text(x1, y1, f'{class_names[j]} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "\n",
    "        gt_label = path.split(sepps)[-2]\n",
    "        pred_label = ','.join(pred_labels) or 'None'\n",
    "        confidence = ','.join(f'{c:.2f}' for c in confidences) or 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "\n",
    "        for j, prediction in enumerate(predictions[0]):\n",
    "            if prediction > threshold:\n",
    "                ax[j].imshow(np.squeeze(image))\n",
    "                ax[j].set_title(f'{class_names[j]} ({prediction:.2f})')\n",
    "                ax[j].axis('off')\n",
    "\n",
    "                predictions = [(class_names[j], x1, y1, x2, y2, prediction)\n",
    "                               for j, prediction in enumerate(predictions[0])\n",
    "                               if prediction > threshold]\n",
    "                for prediction in predictions:\n",
    "                    data.append([os.path.basename(path), gt_label, prediction[0], prediction[5]])\n",
    "\n",
    "        for k in range(j+1, n_cols):\n",
    "            ax[k].axis('off')\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.005)\n",
    "    # plt.show()\n",
    "    plt.savefig('final_image.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    df_split = df.assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "    count = (df_split['Ground Truth Label'] == df_split['Predicted_Label']).sum()\n",
    "    countb = (df_split['Ground Truth Label'] != df_split['Predicted_Label']).sum()\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb9ded-ec50-4366-8a2c-1452789a38de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f77daec5-48df-4fcb-b81e-9141a3c3f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_faults1(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: Pandas dataframe with columns ['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence']\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        pred_labels = []\n",
    "        confidences = []\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                pred_labels.append(class_names[i])\n",
    "                confidences.append(prediction)\n",
    "\n",
    "        gt_label = path.split(os.path.sep)[-2]\n",
    "        pred_label = ','.join(pred_labels) if pred_labels else 'None'\n",
    "        confidence = ','.join([f'{c:.2f}' for c in confidences]) if confidences else 'None'\n",
    "\n",
    "        all_predictions.append((os.path.basename(path), gt_label, pred_label, confidence))\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "    # print(df)\n",
    "    \n",
    "    \n",
    "    # count = len(df[df['Ground Truth Label'] == df['Predicted Label']])\n",
    "    # count1 = len(df[df['Ground Truth Label'] != df['Predicted Label']])\n",
    "    # print('\\nCorrect predictions:',count,'\\nWrong predictions:',count1,'\\n')\n",
    "    \n",
    "#     match_count = df.apply(lambda x: x['Ground Truth Label'] in x['Predicted Label'], axis=1).sum()\n",
    "\n",
    "#     df_split = df.assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "#     split_count = len(df_split[df_split['Ground Truth Label'] == df_split['Predicted_Label']])\n",
    "\n",
    "#     print(f\"Total Ground Truth that match predictions: {match_count}\")\n",
    "#     print(f\"Total predictions that match each Ground Truth: {split_count}\")\n",
    "    \n",
    "    df_split = df.assign(Ground_Truth_Label=df['Ground Truth Label']).assign(Predicted_Label=df['Predicted Label'].str.split(',')).explode('Predicted_Label')\n",
    "\n",
    "    count = len(df_split[df_split['Ground_Truth_Label'] == df_split['Predicted_Label']])\n",
    "    countb = len(df_split[df_split['Ground_Truth_Label'] != df_split['Predicted_Label']])\n",
    "    print(f\"Total predictions that match each Ground Truth: {count}\")\n",
    "    print(f\"Total predictions that do not match each Ground Truth: {countb}\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def detect_faults2(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: List of tuples representing the fault predictions\n",
    "        (fault_name, x1, y1, x2, y2, confidence)\n",
    "    \"\"\"\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        predicted_labels = []\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                x1, y1, x2, y2 = (10 * i, 10 * i, 200 - 10 * i, 200 - 10 * i)\n",
    "                colord = colors[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                label = f'{class_names[i]} ({prediction:.2f})'\n",
    "                ax.text(x1, y1, label,\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "                predicted_labels.append((class_names[i], prediction))\n",
    "\n",
    "        ax.set_title(path.split('\\\\')[-2])\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        true_label = path.split('\\\\')[-2]\n",
    "        predictions_df = pd.DataFrame(predicted_labels, columns=['Predicted Label', 'Confidence'])\n",
    "        predictions_df['Image'] = os.path.basename(path)\n",
    "        predictions_df['Ground Truth Label'] = true_label\n",
    "        predictions_df = predictions_df[['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence']]\n",
    "        all_predictions.append(predictions_df)\n",
    "\n",
    "    all_predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "    # print(all_predictions_df)\n",
    "    return all_predictions_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_faults3(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "\n",
    "    Returns:\n",
    "    - predictions_df: pandas DataFrame representing the fault predictions for all images\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        colors = generate_colors(len(class_names))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(np.squeeze(image))\n",
    "\n",
    "        predicted_labels = []\n",
    "        confidences = []\n",
    "        for i, prediction in enumerate(predictions[0]):\n",
    "            if prediction >= threshold:\n",
    "                x1, y1, x2, y2 = (10 * i, 10 * i, 200 - 10 * i, 200 - 10 * i)\n",
    "                colord = colors[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     fill=False, color=colord)\n",
    "                ax.add_patch(rect)\n",
    "                label = class_names[i]\n",
    "                predicted_labels.append(label)\n",
    "                confidences.append(prediction)\n",
    "                ax.text(x1, y1, f'{label} ({prediction:.2f})',\n",
    "                        fontsize=14, fontweight='bold', color=colord)\n",
    "\n",
    "        ax.set_title(path.split('\\\\')[-2])\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # create a list of dictionaries for each predicted label and confidence\n",
    "        predictions_dict_list = []\n",
    "        for label, confidence in zip(predicted_labels, confidences):\n",
    "            predictions_dict_list.append({\n",
    "                'Image': path.split('\\\\')[-1],\n",
    "                'Ground Truth Label': path.split('\\\\')[-2],\n",
    "                'Predicted Label': label,\n",
    "                'Confidence': confidence\n",
    "            })\n",
    "\n",
    "        # create a DataFrame from the list of dictionaries\n",
    "        predictions_df = pd.DataFrame(predictions_dict_list)\n",
    "\n",
    "        all_predictions.append(predictions_df)\n",
    "\n",
    "    # concatenate all the predictions DataFrames into one\n",
    "    predictions_df = pd.concat(all_predictions)\n",
    "\n",
    "    # reset the index of the DataFrame\n",
    "    predictions_df = predictions_df.reset_index(drop=True)\n",
    "\n",
    "    # print the DataFrame\n",
    "    # print(predictions_df)\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_faults4(model, image_path, class_names, img_size=256, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects faults in a wind turbine blade image using a trained CNN model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained Keras model object\n",
    "    - image_path: File path of the input image or directory\n",
    "    - class_names: List of class names (in order of model output)\n",
    "    - img_size: Image size (default=256)\n",
    "    - threshold: Confidence threshold for predictions (default=0.2)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: DataFrame representing the fault predictions\n",
    "        (Image, Ground Truth Label, Predicted Label, Confidence)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(image_path, list):\n",
    "        image_paths = image_path\n",
    "    elif os.path.isdir(image_path):\n",
    "        image_paths = [os.path.join(root, file)\n",
    "                       for root, dirs, files in os.walk(image_path)\n",
    "                       for file in files\n",
    "                       if file.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    else:\n",
    "        image_paths = [image_path]\n",
    "\n",
    "    all_predictions = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.expand_dims(image, axis=0) / 255.\n",
    "\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "        predicted_classes = [class_names[i] for i, p in enumerate(predictions[0]) if p >= threshold]\n",
    "        # predicted_confidences = [p for i, p in enumerate(predictions[0]) if p >= threshold]\n",
    "        predicted_confidences = [round(p, 2) for i, p in enumerate(predictions[0]) if p >= threshold]\n",
    "\n",
    "\n",
    "        true_class = path.split(os.path.sep)[-2]\n",
    "        image_name = path.split(os.path.sep)[-1]\n",
    "\n",
    "        predictions = [(image_name, true_class, predicted_class, confidence)\n",
    "                       for predicted_class, confidence in zip(predicted_classes, predicted_confidences)]\n",
    "        \n",
    "        \n",
    "\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    df = pd.DataFrame(all_predictions, columns=['Image', 'Ground Truth Label', 'Predicted Label', 'Confidence'])\n",
    "\n",
    "    # print(predictions_df)\n",
    "    \n",
    "    count = len(df[df['Ground Truth Label'] == df['Predicted Label']])\n",
    "    count1 = len(df[df['Ground Truth Label'] != df['Predicted Label']])\n",
    "    print('\\nCorrect predictions:',count,'\\nWrong predictions:',count1,'\\n')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def ossep():\n",
    "    return os.path.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084b448-a312-490c-80f9-48df3884cc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd4f2a28-1e4e-4ec4-9419-d6ac9942c6aa",
   "metadata": {},
   "source": [
    "# Visualizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd78c0c1-c08f-4ea6-9389-c020d99df7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0445f0-2fab-468e-a944-0eec528844ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63776dee-81f4-4a5f-b1ce-6baee6cccda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_probability_distribution_and_roc_curves_one_vs_rest(model_multiclass, X_test, y_test, y_proba):\n",
    "    '''\n",
    "    Plots the probability distributions and the ROC curves for each class of a one vs rest multiclass classification model.\n",
    "\n",
    "    Parameters:\n",
    "    model_multiclass: A multiclass classification model.\n",
    "    X_test: Test set features.\n",
    "    y_test: Test set labels.\n",
    "    y_proba: Predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bins = [i / 20 for i in range(20)] + [1]\n",
    "    classes = model_multiclass.classes_\n",
    "    roc_auc_ovr = {}\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        # Gets the class\n",
    "        c = classes[i]\n",
    "\n",
    "        # Prepares an auxiliar dataframe to help with the plots\n",
    "        df_aux = X_test.copy()\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in y_test]\n",
    "        df_aux['prob'] = y_proba[:, i]\n",
    "        df_aux = df_aux.reset_index(drop=True)\n",
    "\n",
    "        # Plots the probability distribution for the class and the rest\n",
    "        ax = plt.subplot(2, 3, i + 1)\n",
    "        sns.histplot(x=\"prob\", data=df_aux, hue='class', color='b', ax=ax, bins=bins)\n",
    "        ax.set_title(c)\n",
    "        ax.legend([f\"Class: {c}\", \"Rest\"])\n",
    "        ax.set_xlabel(f\"P(x = {c})\")\n",
    "\n",
    "        # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "        ax_bottom = plt.subplot(2, 3, i + 4)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "        plot_roc_curve(tpr, fpr, scatter=False, ax=ax_bottom)\n",
    "        ax_bottom.set_title(\"ROC Curve OvR\")\n",
    "\n",
    "        # Calculates the ROC AUC OvR\n",
    "        roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83416052-7392-4881-8929-b4059ac4ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probability_distribution_and_roc_curves_one_vs_rest(model_multiclass, X_test, true_labels, probas):\n",
    "    '''\n",
    "    Plots the probability distributions and the ROC curves for each class of a one vs rest multiclass classification model.\n",
    "\n",
    "    Parameters:\n",
    "    model_multiclass: A multiclass classification model.\n",
    "    X_test: Test set features.\n",
    "    true_labels: True test set labels.\n",
    "    probas: Predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bins = [i / 20 for i in range(20)] + [1]\n",
    "    classes = model_multiclass.classes_\n",
    "    roc_auc_ovr = {}\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        # Gets the class\n",
    "        c = classes[i]\n",
    "\n",
    "        # Prepares an auxiliar dataframe to help with the plots\n",
    "        df_aux = X_test.copy()\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in true_labels]\n",
    "        df_aux['prob'] = probas[:, i]\n",
    "        df_aux = df_aux.reset_index(drop=True)\n",
    "\n",
    "        # Plots the probability distribution for the class and the rest\n",
    "        ax = plt.subplot(2, 3, i + 1)\n",
    "        sns.histplot(x=\"prob\", data=df_aux, hue='class', color='b', ax=ax, bins=bins)\n",
    "        ax.set_title(c)\n",
    "        ax.legend([f\"Class: {c}\", \"Rest\"])\n",
    "        ax.set_xlabel(f\"P(x = {c})\")\n",
    "\n",
    "        # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "        ax_bottom = plt.subplot(2, 3, i + 4)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "        plot_roc_curve(tpr, fpr, scatter=False, ax=ax_bottom)\n",
    "        ax_bottom.set_title(\"ROC Curve OvR\")\n",
    "\n",
    "        # Calculates the ROC AUC OvR\n",
    "        roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8bbea1c-8fab-4960-a7e0-8e2cf29a8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probability_distribution_and_roc_curves_one_vs_rest(model_multiclass, data_generator, steps):\n",
    "    '''\n",
    "    Plots the probability distributions and the ROC curves for each class of a one vs rest multiclass classification model.\n",
    "\n",
    "    Parameters:\n",
    "    model_multiclass: A multiclass classification model.\n",
    "    data_generator: A data generator that generates test data.\n",
    "    steps: Number of steps to run the generator for.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bins = [i / 20 for i in range(20)] + [1]\n",
    "    classes = model_multiclass.classes_\n",
    "    roc_auc_ovr = {}\n",
    "    \n",
    "    # Generate X_test and y_test using the data generator\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(steps):\n",
    "        batch = next(data_generator)\n",
    "        X_test.extend(batch[0])\n",
    "        y_test.extend(batch[1])\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Get y_proba from the model\n",
    "    y_proba = model_multiclass.predict_proba(X_test)\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        # Gets the class\n",
    "        c = classes[i]\n",
    "\n",
    "        # Prepares an auxiliar dataframe to help with the plots\n",
    "        df_aux = pd.DataFrame(X_test.copy(), columns=['feature_{}'.format(i) for i in range(X_test.shape[1])])\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in y_test]\n",
    "        df_aux['prob'] = y_proba[:, i]\n",
    "        df_aux = df_aux.reset_index(drop=True)\n",
    "\n",
    "        # Plots the probability distribution for the class and the rest\n",
    "        ax = plt.subplot(2, 3, i + 1)\n",
    "        sns.histplot(x=\"prob\", data=df_aux, hue='class', color='b', ax=ax, bins=bins)\n",
    "        ax.set_title(c)\n",
    "        ax.legend([f\"Class: {c}\", \"Rest\"])\n",
    "        ax.set_xlabel(f\"P(x = {c})\")\n",
    "\n",
    "        # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "        ax_bottom = plt.subplot(2, 3, i + 4)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "        plot_roc_curve(tpr, fpr, scatter=False, ax=ax_bottom)\n",
    "        ax_bottom.set_title(\"ROC Curve OvR\")\n",
    "\n",
    "        # Calculates the ROC AUC OvR\n",
    "        roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b2713-8804-45cc-8d80-6f5b2323e820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a80e3e-63ef-4da6-9168-7f63257f9a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "769f5dad-5fb9-4aae-9067-e17377ee4022",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "# Get a list of all currently loaded modules\n",
    "loaded_modules = list(sys.modules.keys())\n",
    "\n",
    "# Print the list of loaded modules\n",
    "print(loaded_modules)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "114950fb-bf92-4495-88af-f3b01bafe67e",
   "metadata": {},
   "source": [
    "import pkg_resources\n",
    "\n",
    "# get a set of all installed packages\n",
    "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "\n",
    "# get a set of all currently loaded modules that are in installed_packages\n",
    "loaded_modules = {mod.__name__ for mod in list(sys.modules.values()) if mod and mod.__name__ in installed_packages}\n",
    "\n",
    "\n",
    "\n",
    "sorted_modules = sorted(loaded_modules)\n",
    "\n",
    "# print the list of loaded modules\n",
    "print(sorted_modules)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14e2cfec-6169-42f7-abe5-ee11b3319d22",
   "metadata": {},
   "source": [
    "import pkg_resources\n",
    "import sys\n",
    "\n",
    "loaded_modules = [m for m in sys.modules.keys() if m in pkg_resources.working_set.by_key]\n",
    "sorted_modules = sorted(loaded_modules)\n",
    "print(sorted_modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754ff75-0b2d-4486-849a-ca35b73b4bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
